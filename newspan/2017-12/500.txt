“All of a sudden we can do the same kind of analysis on images that we have been able to do on text,” said Erez Lieberman Aiden, a computer scientist who heads a genomic research center at the Baylor School of Medicine. He provided advice on one aspect of the Stanford project.
Text has been easier for A.I. to handle, because words have discrete characters — 26 letters, in the case of English. That makes it much closer to the natural language of computers than the freehand chaos of imagery. But image recognition technology, much of it developed by major technology companies, has improved greatly in recent years.
The Stanford project gives a glimpse at the potential. By pulling the vehicles’ makes, models and years from the images, and then linking that information with other data sources, the project was able to predict factors like pollution and voting patterns at the neighborhood level.
“This kind of social analysis using image data is a new tool to draw insights,” said Timnit Gebru, who led the Stanford research effort. The research has been published in stages, the most recent in late November in the Proceedings of the National Academy of Sciences.
In the end, the car-image project involved 50 million images of street scenes gathered from Google Street View. In them, 22 million cars were identified, and then classified into more than 2,600 categories like their make and model, located in more than 3,000 ZIP codes and 39,000 voting districts.
But first, a database curated by humans had to train the A.I. software to understand the images.
The researchers recruited hundreds of people to pick out and classify cars in a sample of millions of pictures. Some of the online contractors did simple tasks like identifying the cars in images. Others were car experts who knew nuances like the subtle difference in the taillights on the 2007 and 2008 Honda Accords.
“Collecting and labeling a large data set is the most painful thing you can do in our field,” said Ms. Gebru, who received her Ph.D. from Stanford in September and now works for Microsoft Research.
Once the car-image engine was built, its speed and predictive accuracy was impressive. It successfully classified the cars in the 50 million images in two weeks. That task would take a human expert, spending 10 seconds per image, more than 15 years.
Other researchers have used Google Street View data for visual clues for factors that influence urban development, ethnic shifts in local communities and public health. But the Stanford project appears to have used the most Street View images in the most detailed analysis so far.
The significance of the project, experts say, is a proof of concept — that new information can be gleaned from visual data with artificial intelligence software and plenty of human help.
The role of such research, they say, will be mainly to supplement traditional information sources like the government’s American Community Survey, the household surveys conducted by the Census Bureau.
This kind of research, if it expands, will raise issues of data access and privacy. The Stanford project only made predictions about neighborhoods, not about individuals. But privacy concerns about Street View pictures have been raised in Germany and elsewhere. Google says it handles research requests for access to large amounts of its image data on a case-by-case basis.
Onboard cameras in cars are just beginning, as auto companies seek to develop self-driving cars. Will some of the vast amounts of image data they collect be available for research or kept proprietary?
To Nikhil Naik, a computer scientist and research fellow at Harvard, who has used Street View images in the study of urban environments, the Stanford project points toward the future of image-fueled research.
