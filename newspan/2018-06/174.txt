The Upshot|If the Robots Come for Our Jobs, What Should the Government Do?
Some big ideas are starting to percolate. But less dramatic ones might work, too.
Lots of smart technologists and futurists are convinced that we are on the cusp of a world in which artificial intelligence, robotics and other technologies will make a large portion of today’s jobs obsolete.
They might be wrong, of course, but the consequences if they’re right would be huge — potentially a defining challenge of the decades ahead, and one that would demand political attention.
Some of the potential answers are big, bold ideas that have gained traction in particular ideological circles. A universal basic income — the idea that the government simply give each citizen enough money every month to support basic needs — has fans among both free-market libertarians and socialists.
But other ideas starting to percolate in economic policy circles may have advantages in terms of cost and political viability.
One interesting entry in this discussion is a paper published Tuesday by the Roosevelt Institute, a liberal think tank, called “Don’t Fear the Robots.” The economist Mark Paul writes that a series of policy steps that are, in isolation, not all that radical would go a long way toward ensuring that the benefits of technological advancement would be widely enjoyed.
As an example, he says the Federal Reserve and other policymakers should commit more energetically to pursuing a “maximum employment” goal set in federal law, even if it means being willing to tolerate a bit more risk of inflation.
Mr. Paul argues for overhauling intellectual property law so that the companies that develop valuable patents and trademarks don’t have such a lengthy monopoly on their innovations. Over time this would probably mean that more of the benefits of technology would accrue to labor rather than capital.
And he sees promise in work-sharing programs like those that have been used to help keep unemployment low in Germany even during economic downturns. The idea is that if a company needs to cut 20 percent of its work force because of new innovations, it is better for society if it cuts each worker’s hours by 20 percent rather than laying off 20 percent of its staff.
Mr. Paul argues that rapid shifts in the skills and technologies demanded by the modern economy strengthen the case for publicly funded higher education and training to help workers adapt.
This set of proposals is based on the idea that the emerging wave of digital disruption won’t result in a permanent loss of demand for workers, but rather shifts in what types of work the economy needs. It’s not unlike early 20th-century America’s shift from an agricultural economy to an industrial one, or its shift from an industrial to an information economy over the last half-century.
While these ideas are coming from a decidedly left-of-center place, it’s striking how some of them overlap with the goals of centrist business interests and even some conservative thinkers.
The McKinsey Global Institute, the research arm of the consulting giant that has produced extensive analysis suggesting that advances in information technology and robotics will endanger millions of jobs in the coming decade, often emphasizes the role of subsidized education and training.
Susan Lund, a partner in the firm, says it is increasingly crucial that people continually upgrade their skills to keep up with changing technology, whether through community colleges, traditional universities or narrowly targeted online training.
Ms. Lund and her McKinsey colleagues also recommend new approaches to making job benefits like health insurance and retirement funds more “portable,” so that people who work as independent contractors or who change jobs frequently can have more stability.
To the degree that many of these ideas imply a more activist government role, conservatives tend to be more leery. But Michael Strain, a scholar at the conservative American Enterprise Institute, says the risks of disruption are high enough that some flexibility may be needed.
In particular, we could be headed toward a bifurcated labor market, where people with advanced skills earn higher wages but where workers without those skills see technology drive down demand for their services, depressing their pay.
By expanding the earned-income tax credit or creating similar programs, for example, the government could increase the effective pay of jobs that otherwise would pay so little that Americans might choose to sit at home rather than take them.
The unemployment rate is currently at its lowest level in 18 years, and the bigger challenge for the economy now is that productivity is too low, not that technology is causing productivity to be so high as to drive people out of jobs.
So this discussion over potential policy responses to a future that may or may not arrive remains speculative — all the more so in an era of congressional dysfunction.
But there’s a recent lesson worth learning from. Globalization and automation caused upheaval in the manufacturing industry from the 1980s through the early 2000s, and millions of factory workers lost their jobs. The disruption to communities is still being felt, and is arguably at the root of a lot of the biggest social and economic problems of this era.
If a similar technological wave is about to wash over millions of service workers, we would all do well to try to keep history from repeating itself.