The researchers compiled information from news organizations, civil society groups and governments to create one of the most comprehensive inventories of disinformation practices by governments around the world. They found that the number of countries with political disinformation campaigns more than doubled to 70 in the last two years, with evidence of at least one political party or government entity in each of those countries engaging in social media manipulation.
“Social media technology tends to empower propaganda and disinformation in really new ways,” said Samantha Bradshaw, a researcher at the Oxford Internet Institute, a department at Oxford University, and co-author of the study. The institute previously worked with the Senate Intelligence Committee to investigate Russian interference around the 2016 campaign.
The report highlights the continuing challenge for Facebook, Twitter and YouTube as they try to combat disinformation, particularly when the perpetrators are governments. The companies have announced internal changes to reduce social media manipulation and foreign interference.
But the research shows that use of the tactics, which include bots, fake social media accounts and hired “trolls,” is growing. In the past two months, the platforms have suspended accounts linked to governments in China and Saudi Arabia.
Ben Nimmo, director of investigations at Graphika, a company that specializes in analyzing social media, said the growing use of internet disinformation is concerning for the 2020 United States election. A mix of domestic and foreign groups, operating autonomously or with loose ties to a government, are building from the methods used by Russia in the last presidential election, making it difficult for the platforms to police, he said.
China’s emergence as a powerful force in global disinformation is one of the most significant developments of the past year, researchers said. The country has long used propaganda domestically, but the protests this year in Hong Kong brought evidence that it was expanding its efforts. In August, Facebook, Twitter and YouTube suspended accounts linked to Beijing that were spreading disinformation about the protests.
There is a new professionalism to the activity, with formal organizations that use hiring plans, performance bonuses and receptionists, he said.
The tactics are no longer limited to large countries. Smaller states can now easily set up internet influence operations as well. The Oxford researchers said social media was increasingly being co-opted by governments to suppress human rights, discredit political opponents and stifle dissent, including in countries like Azerbaijan, Zimbabwe and Bahrain. In Tajikistan, university students were recruited to set up fake accounts and share pro-government views. During investigations into disinformation campaigns in Myanmar, evidence emerged that military officials were trained by Russian operatives on how to use social media.
Most government-linked disinformation efforts were focused domestically, researchers concluded. But at least seven countries had tried to influence views outside their borders: China, India, Iran, Pakistan, Russia, Saudi Arabia and Venezuela.
Ms. Bradshaw said that in the case studies the Oxford team identified, advertising was not central to the spread of disinformation. Instead, she said, the campaigns sought to create memes, videos or other pieces of content designed to take advantage of social networks’ algorithms and their amplifying effects — exploiting the potential for virality on the platforms for free.
Ms. Bradshaw said both government regulation and the steps taken by Facebook to combat this kind of disinformation didn’t go far enough. A lot of the regulation “tends to focus on the content” or “problems at the edges of disinformation problems,” she said, pointing to efforts like Facebook’s transparency in its ads archive.
“But from our research, we know that this problem of microtargeting ads is actually only a very small part of the problems,” Ms. Bradshaw said. Facebook has not addressed deeper structural problems that make it easy to spread false and misleading information, she said.
“To address that you need to look at the algorithm and the underlying business model,” Ms. Bradshaw said.