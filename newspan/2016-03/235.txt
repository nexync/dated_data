What the legendary matches between supercomputer Deep Blue and chess grandmaster Garry Kasparov reveal about today’s artificial intelligence and machine learning fears.
It may not strike everyone as the loftiest ambition: creating machines that are smarter than people. Not setting the bar terribly high, is it? So the more cynical might say. All the same, an array of scientists and futurists are convinced that the advent of devices with superhuman intelligence looms in the not-distant future. The prospect fills some of our planet’s brainiest specimens with dread.
When people of their caliber speak, it seems reasonable to listen. And so, alarms about a computer-spawned apocalypse are a backdrop to the latest installment in the Retro Report series, video documentaries that explore major news events of the past and their continuing effects.
Men of science are not alone in the hand-wringing over the possibility of machines running wild. Asked what they feared most, Americans interviewed by researchers at Chapman University in Southern California ranked the consequences of modern technology near the top. Even death did not rattle them as much; it was way down on their list of worries, at No. 43.
While not discounting that doomsayers may prove someday to be right, Retro Report offers more reassuring views from computer specialists who sense that the end is not nigh — if only, they say, because machines are not nearly as clever, or necessarily as pernicious, as the fretters believe.
Jitters over humanity’s falling victim to various creations are as old Mary Shelley’s Frankenstein monster and the Golem of Jewish tradition. Hostile robots have been on the scene since at least the 1920s with the play “R.U.R.,” by the Czech writer Karel Capek. The initials stood for “Rossum’s Universal Robots.” Indeed, this work introduced “robot” into the language. Since then, run-amok machines have been a science-fiction staple in books and films like “Colossus: The Forbin Project,” “I, Robot,” “2001: A Space Odyssey,” “Transcendence,” “Ex Machina” and the seemingly inexhaustible supply of “Terminator” movies. One sure bet about those films is that, like the Terminator itself, they’ll be back.
On occasion, machines are cast as a benign presence, as in the 2013 film “Her,” in which a man finds intimacy with an operating system that is guided by artificial intelligence (not to mention made alluring by the voice of Scarlett Johansson). In Japan, some people have closely bonded with robot dogs, to the point of holding funerals for automated pooches that cease to function.
More typically, though, the machines — robots, cyborgs, androids, clones — are depicted as threats to human survival. As Retro Report recalls, fear of them in real life grew in 1997 when a chess-playing IBM computer, Deep Blue, defeated the world champion, Garry Kasparov. Apprehension deepened for some in 2011 when two stars of the quiz show “Jeopardy!” were soundly defeated by a new IBM gizmo. (What is Watson?) This week, artificial intelligence will again challenge the human brain as Google’s DeepMind competes in South Korea against a champion in Go, the Chinese board game with trillions of possible moves.
Arguably, there is no reason to lose sleep over those souped-up gadgets. Sure, Watson and its brethren are good at games and other sorts of data processing. But contemplating a takeover of the world’s nuclear arsenals? Not a chance. Nonetheless, some experts foresee a time, not far off, when artificial intelligence, A.I., will match and then exceed human intelligence, at ever-accelerating and frightening speeds.
“Shortly after, the human era will be ended,” Vernor Vinge, a computer scientist and science fiction author, wrote in 1993. That moment, he predicted, would come “within 30 years.” In other words, check your calendars — a mere seven years remain until the arrival of this “technological singularity,” as it was called.
Another A.I. expert, Raymond Kurzweil, has pinpointed 2045 as the due date. Still another student of the subject, James Barrat, also says that once the machines blow past us, man’s reign is through.
Horrific scenarios abound. Superintelligent computers will cause global financial systems to collapse. They will wage war on humans with killer robots far more lethal than today’s drones. They will control nuclear weaponry — think Skynet in the “Terminator” series — to dominate humankind or, worse, wipe it out.
In these grim predictions, the machines always seem to be anthropomorphic: Their instincts are essentially the same as those of humans at their worst; just as people have run roughshod over lower life forms, artificial intelligence networks will abuse their supremacy. Possibilities for them to do good — figuring out how to regenerate human cells, for instance, or creating immunities against disease, or gobbling up carbon dioxide in the atmosphere — tend to get short shrift.
For some analysts, any worry about human survival is theoretical and certainly less immediate than more prosaic, yet vital, concerns. Technological advances have enhanced the ability of governments to spy on their citizens. How to shape policy is now reflected in the struggle between Apple and the Obama administration over access to the iPhone of one of the terrorists in the mass shooting in San Bernardino, Calif., in December.
Economic issues are unavoidable as well. Lawrence H. Summers, president emeritus at Harvard University and a former Treasury secretary, noted that unemployment is disproportionately higher among those whose duties “in various ways have been mechanized.” There are “important consequences for the way the economy is organized and for how fair the economy is,” Mr. Summers said in an interview with Retro Report.
There is, too, a question of how smart robots truly are and whether they can develop superintelligence at the blinding speed envisioned by the more pessimistic forecasters. “Things that are easy for humans are hard for computers,” Guruduth S. Banavar, the director of cognitive computing research at IBM, told Retro Report, “and things that are easy for computers are hard for humans.” Yes, a computer can multiply two numbers of 1,000 digits each in a matter of seconds. But it cannot hold a candle to a toddler when it comes to recognizing faces or performing a task as simple as climbing steps.
Perhaps it is human nature to assume the worst with something new. “Humans often converge around massive technological shifts — around any change, really — with a flurry of anxieties,” Adrienne LaFrance, who covers technology for The Atlantic magazine, wrote a year ago.
