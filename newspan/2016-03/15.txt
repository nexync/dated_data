A landmark 2015 report that cast doubt on the results of dozens of published psychology studies has exposed deep divisions in the field, serving as a reality check for many working researchers but as an affront to others who continue to insist the original research was sound.
On Thursday, a group of four researchers publicly challenged the report, arguing that it was statistically flawed and, as a result, wrong.
The 2015 report, called the Reproducibility Project, found that less than 40 studies in a sample of 100 psychology papers in leading journals held up when retested by an independent team. The new critique by the four researchers countered that when that team’s statistical methodology was adjusted, the rate was closer to 100 percent.
Neither the original analysis nor the critique found evidence of fraud or manipulation of data.
The critique was published in Science, the journal that published the original report. On Thursday, Science also published a strong rebuttal from the authors of the original replication project.
The challenge comes as the field of psychology is facing a generational change, with young researchers beginning to share their data and study designs before publication, to improve transparency. Still, the new critique is likely to feed an already lively debate about how best to conduct and evaluate so-called replication projects of studies. Such projects are underway in several fields, scientists on both sides of the debate said.
One issue the critique raised was how faithfully the replication team had adhered to the original design of the 100 studies it retested. Small alterations in design can make the difference between whether a study replicates or not, scientists say. To address this, Dr. Nosek and his many collaborators consulted closely with the authors of the studies they were trying to reproduce. Afterward, independent researchers — that is, neither from the original study team nor the replication one — evaluated how closely the study designs matched.
But Dr. Wilson and other authors of the critique — Daniel T. Gilbert, Gary King, and Stephen Pettigrew, all of Harvard — pointed out that authors of 31 of the original studies had not explicitly endorsed the design of the retest. They noted that, for example, one study on race initially run at Stanford was replicated in Amsterdam, a different cultural context.
The critique found that the explicitly endorsed studies were nearly four times more likely to replicate than the nonendorsed ones.
Dr. Nosek said he planned to rerun the replications of 11 studies whose authors raised concern to try to answer whether design differences accounted for the differing results.
Another issue that the critique raised had to do with statistical methods. When Dr. Nosek began his study, there was no agreed-upon protocol for crunching the numbers. He and his team settled on five measures, including the strength of the effect and the effect of combining both studies, to look at the results together.
The authors of the critique argued that it would have been better to focus on one measure: How many of the retests would be expected to fail by chance, given the variations, like design differences, introduced by mounting the retests?
Uri Simonsohn, a researcher at the Wharton School of the University of Pennsylvania, has blogged about these issues, including this dispute. He said that the original replication paper and the critique use statistical approaches that are “predictably imperfect” for this kind of analysis.
