Child sexual abuse imagery online is now a problem on an almost unfathomable scale.
It’s not altogether uncommon in investigations for us to turn up information that is shocking and disturbing. The challenge is when, in the course of your reporting, you come across something so depraved and so shocking that it demands attention. People have to know about this, but nobody wants to hear about it.
 In part one, my colleagues Michael Keller and Gabriel Dance on the almost unfathomable scale of the problem — and just how little is being done to stop it.
It’s Wednesday, February 19.
Gabriel, tell me how this investigation first got started.
So it all began with a tip. Early last year, we got a tip from a guy, and this guy was looking up bullets.
Bullets for guns.
Bullets for guns. And he was actually looking for a very specific weight of bullet on Microsoft’s Bing search engine. And while he was looking up these bullets, he started getting results of children being sexually abused.
And the guy was horrified. He didn’t understand why he was seeing these images, couldn’t stand to look at them. And so he reported it to Bing — and heard nothing. And full of outrage, he writes us a letter to our tip line and described what he was looking for, described the kind of images he was getting back. He says, New York Times, can you please look into this for me?
So I actually emailed my colleague Michael Keller and asked him to look into it.
So in the tip, they had listed the search terms they’d used. So we tried to replicate it. We put it into Bing. And we saw a flash on the screen of images of children. And so I wrote back to Gabe and said, yeah, this checks out. You could type words into Bing and get back explicit images of children.
So this is not the dark web. This is just a regular, commonplace search engine.
That’s right. So a few things went through my head. First of all is, what we need to document this. Because, as most of us know, things on the internet change all the time. It’s possible they came down soon after, etc. But we were very unsure what kind of legal liabilities we had when it came to documenting anything regarding this imagery. So we emailed Dave McCraw, who’s the head counsel at The New York Times, to ask him, you know, what can we do? What can’t we do? How do we go about investigating where this imagery is online?
And doing it without somehow violating the law.
That’s right. And David wrote back immediately and said, there is no journalistic privilege when investigating this. You have no protections. And you have to report it immediately to the F.B.I.
And so that’s what we did. We submitted a report both to the F.B.I. and also to the National Center for Missing and Exploited Children, which is the kind of government-designated clearinghouse for a lot of these reports.
They weren’t able to tell us anything about the report we submitted. But it made us wonder, how common is it that they get these kinds of reports? How many images are out there? How many images are flagged to them each year? And they were able to tell us that. And that number was frankly shocking to us.
The handful of images that the tipster stumbled across was just a tiny portion of what the National Center sees every single day. They told us that in 2018 alone, they received over 45 million images and videos.
45 million images a year. That’s more than 120,000 images and videos of children being sexually abused every day. Every single day. But to put it in perspective, 10 years ago, there were only 600,000 images and videos reported to the National Center. And at that time, they were calling it an epidemic.
So in just a decade, it went from 600,000 reports to 45 million?
Yeah. So we were really curious — how does a problem called an epidemic 10 years ago become such a massive issue now?
And one of the first things we learned was that we did try and tackle it back then.
The national epidemic of grown men using the internet to solicit underage teens for sex. As more and more parents become aware of the dangers, so have lawmakers in Washington.
In the mid to late 2000s, as the internet was being more widely adopted, this issue of online child sexual abuse really got on the radar of Congress. There was even a bill being introduced by Debbie Wasserman Schultz.
The internet has facilitated an exploding multibillion-dollar market for child pornography.
There were multiple hearings.
I’m here today to testify about what many of my law enforcement colleagues are not free to come here and tell you.
They heard from law enforcement.
We are overwhelmed. We are underfunded. And we are drowning in a tidal wave of tragedy.
They were overwhelmed with the number of reports that were coming in.
Unless and until the funding is made available to aggressively investigate and prosecute possession of child pornography, federal efforts will be hopelessly diluted.
They in many cases had the tools to see where offenders were, but not enough staff to actually go out and arrest the perpetrators.
We don’t have the resources we need to save these children.
Hello. Thank you for inviting me to speak today. My name is Alicia Kozakiewicz. A Pittsburgh resident, I am 19 years old and a sophomore in college.

For the benefit of those of you who don’t know, don’t remember those headlines, I am that 13-year-old girl who was lured by an internet predator and enslaved by a sadistic pedophile monster. In the beginning, I chatted for months with Christine, a beautiful, red-haired 14-year-old girl, and we traded our school pictures. Too bad that hers were fake. Yeah, Christine was really a middle-aged pervert named John. And he had lots of practice at his little masquerade because he had it all down. The abbreviations, the music, the slang, the clothes — he knew it all. John slash Christine was to introduce me to a great friend of hers. This man was to be my abductor, my torturer. I met him on the evening of January 1, 2002. Imagine, suddenly you’re in the car, terrified, and he’s grabbing onto your hand and crushing it. And you cry out, but there’s no one to hear. In between the beatings and the raping, he will hang you by your arms while beating you, and he will share his prized pictures with his friends over the internet.
The boogeyman is real, and he lives on the net. He lived in my computer, and he lives in yours. While you are sitting here, he’s at home with your children.
Task forces all over this country are poised to capture him, to put him in that prison cell with the man who hurt me. They can do it. They want to do it. Don’t you?
Alicia’s testimony really moved people. People responded. And eventually, about a year later, the bill passes unanimously.
And what is this new law supposed to do?
So this law, the 2008 Protect Our Children Act, is actually a pretty formidable law with some pretty ambitious goals.
It was supposed to, for the first time ever, secure tens of millions of dollars in annual funding for investigators working on this issue. And it required the Department of Justice to really study the problem and put out reports to outline a strategy to tackle it.
And what has happened since this ambitious law was put into place?
In many ways, the problem has only gotten worse. Even though the number of reports has grown into the millions, funding is still pretty much what it was 10 years ago. And even though the government was supposed to do these regular reports, they’ve only done two in 10 years. And that’s an issue, because if you don’t have anyone studying the size of the problem, you don’t have anyone one raising alarm bells and saying, hey, we need more resources for this.
So they didn’t study it. And they didn’t increase the funding in a way that would match the scale at which the problem is growing.
Yeah. It really looked like they had this law in 2008, and then everyone really took their eye off the ball. So we called Congresswoman Debbie Wasserman Schultz, who was one of the leading proponents of this bill, to figure out what happened.
And we are really gobsmacked to hear that she’s unaware of the extent of the failings. She sends a letter to Attorney General William Barr laying out a lot of our findings, requesting an accounting. As far as we know, she hasn’t heard anything.
So even the person who co-wrote the law was unaware that it was pretty much failing.
She knew about the funding, but even she didn’t know that things had gotten this bad.
And we wanted to figure out now, 10 years later, what kind of effect is this having to law enforcement, to the people on the ground working these cases? And what we heard from them really shows what happens when everyone looks away.
Gabriel, Michael — what happens when you start reaching out to law enforcement?
So Mike and I probably spoke with about 20 different Internet Crimes Against Children task forces.  To be honest, most of the time, as an investigative reporter, generally law enforcement — I mean, generally anybody, but especially law enforcement — is not particularly interested in speaking with us. Usually, they don’t see much to gain. But surprisingly, when it came to this issue, they were not only willing to speak with us, but they were interested in speaking with us.
It’s partly because we were using the right terminology. 
And what exactly is the distinction?
Well, legally, they’re basically the same. But for the people who deal with this type of crime day in, day out, who see these images and who speak with survivors, they know that calling it child pornography implies a bunch of things that are generally incorrect.
One is that it equates it with the adult pornography industry, which is legal and made up of consenting adults, whereas children cannot consent to the sexual behavior. The other thing is that the crimes depicted are heinous, and that each one of them is essentially looking at a crime scene.
 But I think they also talked to us, because for the law enforcement who deal with this, they very much feel that the issue is under-covered and under-discussed, especially considering the seriousness of the crime. I mean, we had the kind of coordination and cooperation from these law enforcement officers that we rarely see from anybody whatsoever. They let us go out on raids with them. They provided us with detailed reports. They talked to us about specific cases. They were really, really open, because they felt that as a nation, we were turning our backs on children, essentially.
And once you have that access, what do you find?
What we learned talking with all these law enforcement officers was just how this world operates. A lot of the departments told us about the high levels of turnover they have. We had one commander who said, back when he was an investigator, he saw one image that was so shocking to him, he quit and served a tour in Iraq. That was his escape.
To even see this imagery once changes your life. And these people look at it all day long.  They’re being funded at a level that means they can’t do proactive investigations anymore. So they’re not sitting in chat rooms trying to catch what many of them think are the worst criminals. They’re unable to do anything, really, then respond to the utter flood of reports coming in from the National Center. And because of the sheer number of reports coming in, they’re forced to make some truly terrible decisions on how to prioritize who they’re investigating.
The F.B.I. told us that in addition to, of course, looking for anybody who’s in immediate danger, they have to prioritize infants and toddlers. When we first got into this, we didn’t even consider the idea of infants. And to hear that the F.B.I. — and later L.A.P.D. would say the same thing — we’re prioritizing infants and toddlers, and essentially not able to respond to reports of anybody older than that. I mean, it really left us pretty speechless.
So we’re learning a lot from speaking with law enforcement. But they also only have a small part of the picture. We also are thinking about this tip that we got, where the tipster was able just to find these images on search engines very easily. And so we still have this big question of how easy is it to come across this material.
And how do you go about answering that question?
When we initially started trying to replicate the tipster’s search, we had to stop because we didn’t want to be searching for these illegal images. But then we discovered a technology that would allow us to keep investigating without having to look at the images. It’s called PhotoDNA. And it essentially creates a unique digital fingerprint for each image. And as the National Center is receiving these reports and identifying these illegal images of child sexual abuse, they keep track of these digital fingerprints. And other companies can tap into that database to see, is this image that I have, is it in that database of known images? And we stumbled upon a service from Microsoft that actually allows you to do just that. It’ll take a URL of an image and tell you if it matches an image already in that database. So we wrote a computer program that would replicate that initial search from the tipster, record all of the URLs. The key part of it, though, was that it blocked all images.
So suddenly you can now search for these images, and find where they live on the internet, without illegally looking at them.
Right. So we started doing this test across a number of different search engines.
Bing, from Microsoft.
Another one called DuckDuckGo.
— I mean, I think both our jaws dropped.
We didn’t find any on Google. But on other search engines powered by Microsoft data, we found a number of matches.
Dozens, dozens of images.
You’re saying the company that came up with this technology to help track these images is the very same company whose search engine allows people to find them, view them, keep viewing them.
Right. As soon as we started running this computer program using Microsoft to detect illegal imagery, we were finding illegal imagery on Microsoft. So it was very clear that they were not using their own services to protect users from this type of image.
And Microsoft told us that this was the result of a bug that they fixed. But about a week later, we reran the test and found even more images.
Wow. So it sounds like you’re finding all these images on Microsoft-powered search engines. So how much of this, in the end, is just a Microsoft problem?
It’s more complicated than that. We were performing this limited search just to test search engines on images. But we’re also, at the same time, reading over 10,000 pages of court documents. These are search warrants and subpoenas from cases where people were caught trading this material.
Facebook, Kik, Tumblr.
Google Drive, Dropbox. We read one case where an offender went on a Facebook group to ask for advice from other people — say, hey, how do I share this? How do I get access to it? How do I get access to children? And they say, download Kik to talk to the kids and download Dropbox. And we can share links with you.
And from these documents, it becomes clear that the companies know. I mean, there’s so many cases for each company that they all know. And so the question becomes, what are they doing about it?
Tomorrow on “The Daily,” a victim’s family asks that same question.
Here’s what else you need to know today. President Trump has granted clemency to a new round of high profile individuals, including Bernie Kerik, the former New York City police commissioner who was convicted on eight felony charges, including tax fraud. Michael Milken, the financier convicted of insider trading, and Rod Blagojevich, the former governor of Illinois, who was convicted of trying to sell Barack Obama’s Senate seat after he became president.
Yes, we have commuted the sentence of Rod Blagojevich. He served eight years in jail, a long time.
I watched his wife on television. I don’t know him very well, I met him a couple of times. He was on, for a short while, “The Apprentice,” years ago. Seemed like a very nice person.
And on Tuesday, after a new poll showed him surging in the Democratic primary race, Michael Bloomberg qualified for the next presidential debate, scheduled for tonight in Las Vegas. The poll, conducted by NPR, PBS and Marist College, showed Bloomberg with 19 percent support, putting him in second place behind Bernie Sanders.
 In part one of a two-part series, we look at the almost unfathomable scale of the problem — and just how little is being done to stop it.
Michael H. Keller, an investigative reporter at the The New York Times, and Gabriel J.X. Dance, an investigations editor for The Times.
Images of victims of child sexual abuse recirculate on the internet — seemingly forever. What are tech companies doing to stop this?
From The New York Times, I’m Michael Barbaro. This is “The Daily.”  Today: The role of the nation’s biggest tech companies and why — despite pleas from victims — those illicit images can still be found online.
It’s Thursday, February 20.
I can turn it down, we’ll just get warm.
Last summer, Gabriel and I got on a plane and flew out to the West Coast to meet in a lawyer’s office to speak with the family that she’d been representing.
As Mike said, like, once we started looking into it, there’s so many facets.
So we explained to them a little bit about our reporting and why it was so important that we speak with them.
Yeah, we’re here to answer your questions, too, I mean, as best we can.
And who is this family?
All we can say is that this is a mom and a stepdad who live on the West Coast. And that’s because they only felt comfortable speaking with us if we could protect their privacy.
I mean, we started this not knowing anything about it. And I might get emotional here but — you know, as parents, we’re trying to figure out what’s the best way for our kid to deal with this.
And they started to tell us a story about what happened to their daughter.
So one day, six years ago, the mom is out with her kids, doing some back to school shopping, and she gets a call from the police. And they tell her she has to come in immediately.
It was very weird to get a call.
— to get a phone call from a detective saying, you need to come down to the station right now. We need to talk to you. We feel your kids are in danger. And I’m like, what?
She called me panicked, going they want to talk to us. Go talk to them. We don’t have anything — we’re not criminals, so.
So she goes into the sheriff’s office.
And there’s this F.B.I. agent, introduces himself. He said, you know, we think your children are in danger from their father, particularly the youngest one. And I was just shocked and had no idea what they were talking about. No idea. Mind you, I had my two other kids who I was shopping with, were in the room next door, playing or doing something, I don’t know what they do. And he just went on to say that we’re going to start investigating him, or we’ve been investigating him.
And there’ll be an agent at our house Friday to tell us more.
And she showed up Friday morning, August 23, 9:00 in the morning. Can I talk to you outside? And we talk outside. And she drops this bomb on us.
And what the agent tells her is that her children’s biological father had been sexually abusing her youngest child, starting at the age of four years old and going on for four years. And not only that, he had been documenting this abuse with photos and videos and had been posting them online.
So I remember that moment. I mean, I think I kind of doubled over or something. I was just shocked. I asked, when did this start? How long has this been going on? How did I not know? I’m mom. So all of those questions and all those feelings, and I mean everything just came crashing down.
What does this couple tell their children at this point?
The F.B.I. agent said, actually, it’s better if I’m the one to tell your kids.
In her experience, it would be best for the news to come from her, as opposed for me telling the kids directly that their dad was arrested, because kids might blame me and point the finger. I said, yeah, whatever you think.
She said, you know, your dad has been arrested. I don’t think we even talked about what it was for or why he was arrested, just that he was arrested and they’re still investigating.
But it isn’t your fault.
(CRYING) And how could somebody do that to their own child?
I still — I don’t think I’ll ever understand a person like that.
— and has a counselor, and is not only dealing with all of the normal things that young teenagers have to deal with, but the trauma of being sexually abused at such a young age.
So that’s something she’s just now having to learn how to say no, how to inform others that she’s not comfortable with something. And she’s only 13.
And meanwhile, even though the physical abuse ended six years ago, the images continue to circulate to this day. And the parents know that because they get notified whenever someone is arrested having these images in their possession.
And why would they be notified? What’s the point? It feels like it would just be a horrendous reminder every time that this is still happening.
One of the abilities that victims of this crime have is to seek restitution from the offenders. So when they’re notified, they and their lawyers can petition the court to get a certain sum of money, which is a really good thing. And helps to rectify some of the damage that was done.
But Mike, you’re right.
Oh my gosh, this person in Kansas and New York. Somebody in Wisconsin saw my kid, and oh my god.
It is a brutal, double-edged sword, these notifications.
Oh my god. There’s people out there who know about this, and they can watch it over and over.
And for this young woman, her parents and their lawyer received more than 350 notices in just the last four years.
Wow. So something on the order of 100 times a year, somebody is convicted of having looked at these photos of their daughter?
What do you think should be kind of in the front of our minds? What do you think is really important for us to understand?
I don’t know enough about technology, but I just — where is this crap, you know?
How do we get it off?
— why can’t they just take it down?
How do we, how do we make it go away?
And that’s the same question we had. Why, six years later, are these images and videos still showing up on some of the largest technology platforms in the world?
Figure it out. We’ve got this technology. We can go to the moon and Mars and stuff. 
And are the companies doing enough to prevent it?
Gabriel, Michael, how do you begin to answer these parents’ very reasonable questions about why these images of their child’s sexual abuse keep showing up online?
So before we can answer the question of why these images keep showing up, we needed to understand where the images were online and what companies were responsible for people sharing them. But the National Center, which you’ll remember is the designated clearinghouse for this information, wouldn’t tell us.
But they know, right? So why wouldn’t they tell you? Why wouldn’t they give you that information?
Part of the reason why they don’t divulge these numbers is because the companies are not required by law to look for these images.
It’s voluntary to look for them.
Right. Without the help of these companies, they have no idea where these images are, where they’re coming from, how many of them there are.
That’s right. And the National Center is concerned that if they disclose these numbers, that they might damage those relationships which they depend on.
But then we start to hear anecdotally that there is one company responsible for the majority of the reports. And that was Facebook. So we are doing everything we can to run down that number. But very few people know it. However, we ultimately do find somebody who has documentation that reveals that number.
So in 2018, the National Center received over 18.4 million reports of illegal imagery. And the number this person provides us shows that of those 18.4 million, nearly 12 million came from Facebook Messenger.
Wow. So the vast majority of them.
Almost two out of every three reports came from Facebook Messenger.
So just that one service of Facebook.
That’s right, just the chat application. This doesn’t include groups or wall posts or any of the other public information that you might post or share. This is specifically from the chat application.
But then, after we reported that number, the D.O.J. actually comes out and says that Facebook in total — so Messenger plus the other parts of the platform — are responsible for nearly 17 of the 18.4 million reports that year.
Wow. This is a Facebook problem.
That’s what the numbers at first glance would suggest. But we realized we needed to talk to people that really understood this to know what conclusions to come to from these numbers.
So we called up somebody who would know better than almost anybody.
Gabe Dance. Mike Keller here.
I’m doing OK. I’m getting back to my office.
Alex was the former chief security officer at Facebook for the past several years. He’s now a professor at Stanford University.

You’ve obviously worked on this area for years. Facebook is, Facebook Messenger is responsible for about 12 million of the 18.4 million reports last year to the National Center. That seems like a lot. Can you help us understand what’s happening on that platform?
That’s not because Facebook has the majority of abuse. It’s because Facebook does the most detection.
It’s them working the hardest to find this type of content and report it.
I expect that pretty much any platform that allows the sharing of files is going to be absolutely infested with child sexual abuse. If everybody was doing the same level of detection, we’d probably be in the hundreds of millions of reports.
 That of the 18 million, the reason why Facebook has so many is because other companies are not reporting this at all?
Essentially, yes. What he’s saying is that Facebook is reporting such a high number of images and videos because they’re looking for them. And that a lot of other companies aren’t even doing that.
Facebook checks effectively every image that transfers across the platform in an unencrypted manner to see whether or not it is known child sexual abuse material.
 And in doing so, Facebook is finding far, far more of this content than any other company.
So he’s saying don’t shoot the messenger here.
That’s what he’s saying. So as of now, this is the best method these companies have to identify and remove the imagery.
Mhm. What Facebook is doing?
So why doesn’t every technology company do this? It seems pretty straightforward.
Well, the short answer is that it’s not baked into the business model.
That this is not something that helps them grow their base of users. It’s not something that provides any source of revenue. And it’s in fact something that works against both of those things, if done correctly.
You’re deleting your own users.
That’s right. You’re deleting your own users. And it costs money to delete your own users.
Right. You have to spend money to hire people to find accounts that are doing something wrong that you’re then going to lose as a customer.
You got it. So both of those things fly in the face of almost all of these companies’ business models. And Stamos actually told us something else interesting.
The truth is that the tech industry is still pretty immature at the highest levels about the interaction between executives.
And that’s that these companies aren’t really working together to solve this problem.
 But they understand that their boats all rise and fall together, and so they are able to work together on what kind of regulation they’d like to see. But a lot of the top executives at the tech companies really kind of personally despise one another. And it is very difficult to get them to agree to anything from a policy perspective.
And in our reporting, we found some pretty egregious disparities in how different companies police this on their platforms. Amazon, who has a massive cloud storage business, for example — they handle millions of uploads and download a day — they don’t scan whatsoever. Apple has an encrypted messaging platform, so that doesn’t get scanned. They also choose not to scan their photos in iCloud. Snapchat, Yahoo, they don’t scan for videos at all, even though everyone knows video is a big part of the problem. And it’s not because the solutions don’t exist, they just have chosen not to implement them.
And now Facebook, the company looking for this content most aggressively, is starting to rethink its policy in doing that. Over the last few years, a lot of tech companies have realized that privacy is an important feature that a lot of their customers are expecting. And citing privacy concerns, Facebook announced that it will soon encrypt its entire Messenger platform, which would effectively blind them to doing any type of automated scanning between chat — which, again, was responsible for nearly 12 million of those 18.4 million reports.

Right. They would limit their own ability to be aware of it.
And privacy is important enough that they would handicap their ability to find this criminal conduct and these horrible photos?
Based on the criticism that a lot of tech companies have received over the last few years, moving towards encryption is a really attractive option for them. Because it lets them say, we really care about the privacy of your conversations, and we’re going to make that more secure.
Gabriel, it occurs to me that the debate over privacy is enormous and loud. We’ve done episodes of “The Daily” about it, many episodes of “The Daily” about it.  It’s not as widely debated. It’s the first time we’re talking about it. Is that reflected in this decision, the attention that these two subjects get?
It is. And in fact, it’s one of the main reasons we chose this line of reporting. The issue of child sexual abuse imagery really brings the privacy issue to a head, because we’re forced with these very, very stark decisions that we’re discussing here right now. Which is that, is it more important to encrypt the communications on a platform where it’s well known that children interact with adults? Is it worth it to encrypt those communications to protect people’s privacy when we know what the ramifications for children are?
But the child protection question is also a privacy question. And when you are Facebook and you’re saying, we’re going to encrypt conversations for the privacy of our users, the child advocates will say, well, but you’re doing nothing to protect the privacy of the child in the image.

And this means that next year, or whenever Facebook moves ahead with its plan to encrypt, they won’t be sending nearly 17 million reports to the National Center. They’ll be sending far fewer.
And that means for the family that we spoke with on the West Coast, who is receiving about 100 notifications a year that someone has been caught with images and videos of their daughter, they’ll likely be getting far fewer notifications. But not because people aren’t looking at these images. They still will be — the family just won’t know about it.
That’s my big thing. I want people to care about this, because there is a human factor to this obviously. We have to force people to look at it, you know, the tech companies. They have to do something about it.
So where does that leave this family now?
They feel abandoned and angry with the tech companies.
They have to do something about it, because knowing that that’s out there, I don’t know, it’s just being traumatized all over again.
They keep getting these notifications that their daughter’s imagery has been found.
It’s ongoing. It’s lifelong. There’s nothing they can do about being a victim for the rest of their life.
And the way they describe it is it’s like getting hit by a truck.

Only to get back up and get hit by a truck time after time after time.
There’s no, there’s no other way to say it. She’s — that will be there forever until it’s removed, until somebody comes up with a way to take that off.
— they’re not telling their daughter. They’re not going to tell her that her images are online.
Making the team. She worries about sleepovers and wrestling, grades.
She doesn’t need to be worrying about the worst part of her life available on the internet.
— when they do a Google search for a bullet, up pops her image. That’s horrible.
But there is a clock ticking.
I just found out when she turns 18, it isn’t a choice. The F.B.I. will get ahold of her, because she’s an adult victim now.
When she turns 18, the federal government is going to start sending her the notifications. So what they’re hoping is that in the four years until that happens, the tech companies are going to solve this problem.
Which is to say, get these images offline for good.
That’s what they hope.
My motivation for this is we have to explain to our daughter, this is on the internet. And she has to live with that. But being able to tell her — you know, if you can tell me in five years, this will be something that was, not is, that’d be great.
The dad asked us the question, can you tell me that when I talk to my daughter about this when she turns 18, that I can tell her that these horrific images used to be online but no longer are?
What I wished I could tell them was yes.
What I did tell him was that that was the point of us doing this reporting. Was the hopes that something would change, and that in five years, those images would no longer be there.
But from everything we’ve learned, it’s only getting worse.
Yeah, you know, put yourself in that kid’s shoes.
What if you were the person they’re looking at the rest of your life? If we can tell that this happened, instead of this is happening, the world would be a lot better off. She’ll be a lot better off.
Michael, Gabriel, thank you.
A few weeks ago, the National Center for Missing and Exploited Children released new data for 2019. It said that the number of photos, videos and other materials related to online child sexual abuse grew by more than 50 percent to 70 million.
For highlights and analysis of Wednesday night’s Democratic debate in Las Vegas, the first to include former New York City Mayor Michael Bloomberg, listen to this morning’s episode of “The Latest.” You can find it on “The Daily” feed or by searching for “The Latest,” wherever you listen.
“The Daily” is made by Theo Balcomb, Andy Mills, Lisa Tobin, Rachel Quester, Lynsea Garrison, Annie Brown, Clare Toeniskoetter, Paige Cowett, Michael Simon Johnson, Brad Fisher, Larissa Anderson, Wendy Dorr, Chris Wood, Jessica Cheung, Alexandra Leigh Young, Jonathan Wolfe, Lisa Chow, Eric Krupke, Marc Georges, Luke Vander Ploeg, Adizah Eghan, Kelly Prime, Julia Longoria, Sindhu Gnanasambandan, Jazmín Aguilera, M.J. Davis Lin, Austin Mitchell, Sayre Quevedo, Neena Pathak, Dan Powell, Dave Shaw, Sydney Harper, Daniel Guillemette, Hans Buetow, Robert Jimison and Michael Benoist. Our theme music is by Jim Brunberg and Ben Landsverk of Wonderly. Special thanks to Sam Dolnick, Mikayla Bouchard, Stella Tan, Lauren Jackson, Julia Simon, Mahima Chablani and Nora Keller.