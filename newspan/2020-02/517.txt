Though a breakthrough for law enforcement, the technique could allow the little-known start-up to collect an extraordinarily sensitive set of data and images.
Law enforcement agencies across the United States and Canada are using Clearview AI — a secretive facial recognition start-up with a database of three billion images — to identify children who are victims of sexual abuse. It’s a powerful use case for the company’s technology, but raises new questions about the tool’s accuracy and how the company handles data.
Investigators say Clearview’s tools allow them to learn the names or locations of minors in exploitative videos and photos who otherwise might not have been identified. In one case in Indiana, detectives ran images of 21 victims of the same offender through Clearview’s app and received 14 IDs, according to Charles Cohen, a retired chief of the state police. The youngest was 13.
“These were kids or young women, and we wanted to be able to find them to tell them we had arrested this guy and see if they wanted to make victim statements,” Mr. Cohen said.
Another official, a victim identification officer in Canada, who was not authorized to discuss investigations publicly, described Clearview’s technology as “the biggest breakthrough in the last decade” in the field of child sexual abuse crimes.
But privacy advocates say the company’s database is untested and unregulated, and could cause new kinds of harm. Clearview stores pictures uploaded by investigators — known as probe images — on its servers, meaning it could amass an extraordinarily sensitive data set of child victims of sexual abuse and exploitation.
According to a company document distributed to clients, “searches are retained forever” by default, but administrators can change their settings so search images are purged after 30 days.
Clearview operated largely in the shadows until a New York Times report last month revealed its use by local and federal law enforcement agencies across the country. The company has harvested billions of photos of individuals from the public internet, including sites such as Facebook, Twitter, Venmo and YouTube. When a user uploads a person’s photo to Clearview, the app returns other images of the person and the web addresses where they appeared.
In numerous publicity documents, Clearview promotes the use of its technology by law enforcement to solve child sexual abuse cases. But until recently, the company focused on its role in identifying perpetrators, not victims.
Critics of Clearview said the benefits of such a database did not outweigh its harms.
Ms. O’Sullivan said she was concerned that Clearview’s software had not been tested for accuracy by an independent agency. Facial recognition algorithms can work poorly on young people, partly because their faces change as they age, and partly because children are often not included in the data sets used to train the algorithms.
Ms. O’Sullivan noted that if the tool made an incorrect match, there could be devastating effects for wrongly identified children and their families.
“The exchange of freedom and privacy for some early anecdotal evidence that it might help some people is wholly insufficient to trade away our civil liberties,” she said.
Law enforcement is required to verify each identity when it uses the Clearview app, Mr. Ton-That said in his email. But he could not say how many children were in its database.
The app is being used by task forces in Florida, Indiana and South Dakota dedicated to investigating child abuse, as well as by the Department of Homeland Security and law enforcement in Canada.
Like several officers who spoke with The Times, the Canadian investigator was reluctant to discuss Clearview, fearing offenders would shift their tactics.
There are legal risks associated with handling this type of imagery. It would be against the law for the company to receive images of abuse without immediately informing the authorities and deleting the material from its servers. Mr. Ton-That said Clearview’s app transmitted only faces, not entire images.
The Times verified this behavior by analyzing a version of Clearview’s Android app, but was not able to examine the company’s iOS offering or a web-based version.
None of the law enforcement agencies The Times spoke with would say whether they had performed a technical audit of Clearview before using the software. Nor would any respond to questions regarding the specific use of the application, saying they did not comment on investigative techniques.
Britney Walker, a spokeswoman for the Department of Homeland Security’s Child Exploitation Investigations Unit, said that it collaborated with external agencies to assist in investigations, but that the unit’s “victim-centered” approach forbade any sharing of illegal imagery.
“Under no circumstances would the agency share child sexual abuse materials to private companies,” Ms. Walker said.
 Johann Hofmann, the chief executive of Griffeye, said the company’s imagery analysis software was installed inside law enforcement networks and was designed to avoid sending images to third parties, including Griffeye itself.
Still, Mr. Burns said, he understood why investigators would want to use facial recognition software. “They are faced with a very grim task, and if there’s a tool that gives them an opportunity to safeguard victims, I don’t blame them for trying to grab it with both hands,” he said.
Since Clearview’s practices have come to light, Facebook, LinkedIn, Twitter, Venmo and YouTube have sent the company cease-and-desist letters, asking it to stop scraping photos from their sites and delete existing images in its database. The attorney general of New Jersey banned the use of Clearview by officers in the state and called for an investigation into how it and similar technologies were being used by law enforcement. A lawsuit seeking class-action certification was filed in Illinois, where a strong biometric privacy law prohibits the use of residents’ faceprints without their consent, and another was filed in Virginia on Monday.
Bills that would ban the use of facial recognition by the police have recently been introduced in New York and Washington. And Clearview received a letter from Senator Edward Markey, Democrat of Massachusetts, asking for a list of law enforcement agencies that have used the app and whether biometric information has been collected for children under 13 years old.
Many agencies had already been using Clearview for months, but the letter made no mention of that.
Michael H. Keller and Aaron Krolik contributed reporting.