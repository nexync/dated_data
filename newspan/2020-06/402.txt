When is research considered reliable? The answer isn’t always fully known. Here’s the approach our journalists take in evaluating studies and their results.
After early studies showed promising results of the anti-malarial drug hydroxychloroquine in coronavirus patients, President Trump quickly promoted it as a possible treatment and later announced that he was taking the drug as a preventive measure.
But publishers of the study from France that Mr. Trump had referenced said that it fell short of their standards, while researchers in Brazil examining the related drug chloroquine halted their research after patients given high doses developed potentially fatal heart arrhythmias. Throughout its coverage, The Times has cited scientists’ reservations about the drug’s effectiveness and reported last week that the first controlled study of the drug found it did not prevent infections in people exposed to the virus.
The misjudgments about hydroxychloroquine underscore the importance of how reporters at The Times cover scientific research: They study the study, and tell readers what’s known and what’s not. Even as scientists work feverishly to answer questions about the pandemic, science reporters carefully parse fact from conjecture, truth from folly.
That’s necessary now especially because “there’s a flood of new science, much of it seat-of-the-pants,” said Celia W. Dugger, the health and science editor.
Before you read about a study in The Times, reporters will have looked into the researchers’ backgrounds and often consulted three to five outside experts to determine the quality of the work. Reporters also ask questions like: What are the margins of error? Did the study include enough patients to get meaningful results? And what are the shortcomings of the research?
Historically, reporters have considered studies published by major science journals — like Nature, The Lancet and The New England Journal of Medicine — to be the most reliable, because those publications use expert editors and rigorously vet research methods and conclusions by sending them to other scientists to evaluate.
But last week, both The Lancet and The New England Journal of Medicine retracted big coronavirus studies because they were based on data that could not be verified. The Times had reported on one of those studies.
Cases like this highlight the need for vigilance. Before citing such research, Times reporters will generally seek out independent experts to comment. That’s critical now because during the pandemic, the length of time from experiment to published study has been “short-circuited,” said Mike Mason, a deputy science editor, adding that what once took as long as a year and a half has been shortened in some cases to weeks.
Now many researchers post their work online in what is known as a preprint, or a study that gets released without the standard practice of peer review. Some preprints contain observational or anecdotal work that doesn’t meet more exacting scientific standards like randomly selected patients or control groups enabling more definitive conclusions. Times reporters who write about preprints try to make clear to readers what the research shows and what it leaves unanswered.
Those outside the field have rarely paid attention to these preprint studies — until now. Politicians and everyday citizens sometimes look to studies in their infancy.
The Times’s guidelines recognize that reporters must balance the weight that they give to such studies with the need to provide information about those that wind up in the global conversation.
