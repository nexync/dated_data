If you’re wondering why you haven’t read or heard anything about a deadly explosion in Bangkok on Tuesday, it’s because there wasn’t a deadly explosion in Bangkok on Tuesday. And the resulting confusion illustrated a downside of Facebook’s switch to automating tasks once left up to the judgment of humans.
On the Safety Check page automatically generated for the “explosion,” Facebook linked to a bogus article, an aggregated version of the bogus article and an article about Thai street food, according to Khaosod English. The mistaken stories appeared to be referencing a bombing at a religious shrine that happened in 2015.
On Wednesday, Facebook stood by the activation of the feature, pointing not to the 2015 bombing but to local media reports of a protest on Tuesday in which there were no injuries.
A man indeed threw five firecrackers from the roof of a bank toward the Government House building in Bangkok on Tuesday, apparently in a bid to get attention, according to The Bangkok Post. Onlookers ran away in panic, the English-language newspaper reported.
The shouting man appeared to be seeking help with a financial problem, Khaosod English reported. The protest never rose to the attention of international media, but Facebook said enough people were discussing it to trigger the Safety Check.
At its best, the Facebook tool can be an efficient way to relieve concerns for friends and loved ones who might be in danger, based on their location data and profile information. In its first two years, Facebook activated it 39 times for major events like the earthquake in Nepal and the terror attacks in Paris. People in the dangerous areas are prompted to check in as safe, an action that would send a notification to their friends who would likely be worried.
For those two years, Facebook employees decided when a situation warranted Safety Check to be activated, but that opened the company to criticism of how it determined what a worthy event would be. Some wondered why the feature was turned on for attacks in Paris, Brussels and Orlando, but not for attacks in Iraq.
Now, humans don’t have to make those calls. In June, Facebook began rolling out automated, “community-generated” Safety Checks.
“As with all Safety Check activations, Facebook relies on a trusted third party to first confirm the incident and then on the community to use the tool and share with friends and family,” a Facebook spokeswoman said in a statement.
Under the new system, the “third party,” which Facebook did not identify, alerts the social-media platform after what it perceives to be a major incident, and the tool is rolled out after an unspecified number of people discuss the event on Facebook. The incident’s page is automatically titled based on the third-party alert, Facebook said.
The result has been a major increase in the use of the tool. Facebook said more than 350 Safety Checks have been activated since June, almost nine times the total of the previous two years.
The Bangkok “explosion” had been confirmed by the third party based on two local media reports about the firecracker protest, Anna White, a Facebook spokeswoman, said in an email. She acknowledged that the bogus stories should not have been featured on the automatically generated page, but said they were not a factor in the activation of the Safety Check.
Confusion over the events in Bangkok comes as Facebook wrestles with the question of how to handle so-called “fake news,” a scourge of deliberately misleading or false articles that the social network’s algorithms can rocket into prominence.
In August, Facebook reduced the role of a team dedicated to curating the site’s Trending Topics, opting for automation over editorial judgment and handwritten headlines. Critics had accused the employees of intentionally limiting the appearance of conservative news sources, which Facebook denied.
Since then, the Trending Topics feature has been criticized for repeatedly surfacing fake news.