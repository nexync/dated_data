A police officer on the late shift in an Ohio town recently received an unusual call from Facebook.
Earlier that day, a local woman wrote a Facebook post saying she was walking home and intended to kill herself when she got there, according to a police report on the case. Facebook called to warn the Police Department about the suicide threat.
Police stations from Massachusetts to Mumbai have received similar alerts from Facebook over the last 18 months as part of what is most likely the world’s largest suicide threat screening and alert program. The social network ramped up the effort after several people live-streamed their suicides on Facebook Live in early 2017. It now utilizes both algorithms and user reports to flag possible suicide threats.
Facebook’s rise as a global arbiter of mental distress puts the social network in a tricky position at a time when it is under investigation for privacy lapses by regulators in the United States, Canada and the European Union — as well as facing heightened scrutiny for failing to respond quickly to election interference and ethnic hatred campaigns on its site. Even as Facebook’s chief executive, Mark Zuckerberg, has apologized for improper harvesting of user data, the company grappled last month with fresh revelations about special data-sharing deals with tech companies.
The anti-suicide campaign gives Facebook an opportunity to frame its work as a good news story. Suicide is the second-leading cause of death among people ages 15 to 29 worldwide, according to the World Health Organization. Some mental health experts and police officials said Facebook had aided officers in locating and stopping people who were clearly about to harm themselves.
“In the last year, we’ve helped first responders quickly reach around 3,500 people globally who needed help,” Mr. Zuckerberg wrote in a November post about the efforts.
Facebook said it worked with suicide prevention experts to develop a comprehensive program to quickly connect users in distress with friends and send them contact information for help lines. It said experts also helped train dedicated Facebook teams, who have experience in law enforcement and crisis response, to review the most urgent cases. Those reviewers contact emergency services only in a minority of cases, when users appear at imminent risk of serious self-harm, the company said.
Facebook said its suicide risk scoring system worked worldwide in English, Spanish, Portuguese and Arabic — except for in the European Union, where data protection laws restrict the collection of personal details like health information. There is no way of opting out, short of not posting on, or deleting, your Facebook account.
A review of four police reports, obtained by The Times under Freedom of Information Act requests, suggests that Facebook’s approach has had mixed results. Except for the Ohio case, police departments redacted the names of the people flagged by Facebook.
In one case in May, a Facebook representative helped police officers in Rock Hill, S.C., locate a man who was streaming a suicide attempt on Facebook Live. On a recording of the call to the police station, the Facebook representative described the background in the video — trees, a street sign — to a police operator and provided the latitude and longitude of the man’s phone.
The Police Department in Mashpee, Mass., had a different experience. Just before 5:16 a.m. on Aug. 23, 2017, a Mashpee police dispatcher received a call from a neighboring Police Department about a man who was streaming his suicide on Facebook Live. Officers arrived at the man’s home a few minutes later, but by the time they got to him, he no longer had a pulse, according to police records.
At 6:09 a.m., the report said, a Facebook representative called to alert the police to the suicide threat.
Facebook’s Ms. Cain said that, in some cases, help unfortunately did not arrive in time. “We really feel for those people and their loved ones when that occurs,” she said.
The fourth case, in May 2017, involved a teenager in Macon, Ga., who was streaming a suicide attempt. Facebook called the police after officers had already found the teenager at her home, a spokeswoman for the Bibb County sheriff’s office said. The teen survived the attempt.
The Department of Veterans Affairs has developed a suicide risk prediction program that uses A.I. to scan veterans’ medical records for certain medicines and illnesses. If the system identifies a veteran as high risk, the V.A. offers mental health appointments and other services. Preliminary findings from a V.A. study reported fewer deaths over all among veterans in the program compared with nonparticipating veterans.
In a forthcoming article in a Yale law journal, Dr. Mason Marks, a health law scholar, argues that Facebook’s suicide risk scoring software, along with its calls to the police that may lead to mandatory psychiatric evaluations, constitutes the practice of medicine. He says government agencies should regulate the program, requiring Facebook to produce safety and effectiveness evidence.
“In this climate in which trust in Facebook is really eroding, it concerns me that Facebook is just saying, ‘Trust us here,’” said Dr. Marks, a fellow at Yale Law School and New York University School of Law.
