As a young social psychologist, she played by the rules and won big: an influential study, a viral TED talk, a prestigious job at Harvard. Then, suddenly, the rules changed.
I first met Amy Cuddy in January, soon after she moved into a new office at the Harvard School of Public Health. Cuddy was, at the time, officially on the faculty at Harvard Business School, but she was taking a temporary leave, her small box of an office filled with boxes. As she talked about her life in recent years, my attention kept drifting to her left arm, which she had trapped underneath her right leg, which crossed the left. She was slightly hunched over, and yet her right arm, long and lean — she danced for many years — gesticulated freely and expressively, so that the contrast gave the impression of someone in a conflicted emotional state, someone both wanting to tell her story and unsure about doing so.
That visual might have escaped me altogether, except that Cuddy, a social psychologist, is best known to the public for her work on body language. And hers seemed to embody a divide that had characterized her life in the last couple of years, a sense of two selves, one highly sensitive, the other more confident, even skilled in the art of conveying that confidence.
Cuddy became famous in her field for a 2010 study about the effects of “power poses.” The study found that subjects who were directed to stand or sit in certain positions — legs astride, or feet up on a desk — reported stronger “feelings of power” after posing than they did before. Even more compelling than that, to many of her peers, was that the research measured actual physiological change as a result of the poses: The subjects’ testosterone levels went up, and their cortisol levels, which are associated with stress, went down.
The video is now TED’s second-most popular, having been seen, to date, by some 43 million viewers. In the years after the talk, Cuddy became a sought-after speaker, a quasi celebrity and, eventually, the author of a best-selling book, “Presence,” in 2015. The power pose became the sun salutation for the professional woman on the cusp of leaning in. Countless hopefuls, male and female, locked themselves in bathroom stalls before job interviews to make victory V’s with their arms; media trainers had their speakers dutifully practice the pose before approaching the stage. Cuddy has gone on to give talks on power and the body (including power posing) and stereotyping to women’s groups in Australia, at youth homeless shelters, to skin-care workers by the thousands, to employees at Target and agents at State Farm Insurance. Cuddy’s fans approach her in airports, on ski slopes in Telluride, in long lines after her talks, to hug or to thank her, filled with their own power-posing stories — sharing how bold body language helped them get their jobs or win some match or confront a bully at work.
But since 2015, even as she continued to stride onstage and tell the audiences to face down their fears, Cuddy has been fighting her own anxieties, as fellow academics have subjected her research to exceptionally high levels of public scrutiny. She is far from alone in facing challenges to her work: Since 2011, a methodological reform movement has been rattling the field, raising the possibility that vast amounts of research, even entire subfields, might be unreliable. Up-and-coming social psychologists, armed with new statistical sophistication, picked up the cause of replications, openly questioning the work their colleagues conducted under a now-outdated set of assumptions. The culture in the field, once cordial and collaborative, became openly combative, as scientists adjusted to new norms of public critique while still struggling to adjust to new standards of evidence.
Cuddy, in particular, has emerged from this upheaval as a unique object of social psychology’s new, enthusiastic spirit of self-flagellation — as if only in punishing one of its most public stars could it fully break from its past. At conferences, in classrooms and on social media, fellow academics (or commenters on their sites) have savaged not just Cuddy’s work but also her career, her income, her ambition, even her intelligence, sometimes with evident malice. Last spring, she quietly left her tenure-track job at Harvard.
Some say that she has gained fame with an excess of confidence in fragile results, that she prized her platform over scientific certainty. But many of her colleagues, and even some who are critical of her choices, believe that the attacks on her have been excessive and overly personal. What seems undeniable is that the rancor of the critiques reflects the emotional toll among scientists forced to confront the fear that what they were doing all those years may not have been entirely scientific.
One of the seminal social-psychology studies, at the turn of the 20th century, asked a question that at the time was a novel one: How does the presence of other people change an individual’s behavior? Norman Triplett, a psychologist at Indiana University, found that when he asked children to execute a simple task (winding line on a fishing rod), they performed better in the company of other children than they did when alone in a room. Over the following decades, a new discipline grew up within psychology to further interrogate group dynamics: how social groups react in certain circumstances, how the many can affect the one.
Since the late 1960s, the field’s psychologists have tried to elevate the scientific rigor of their work, introducing controls and carefully designed experiments like the ones found in medicine. Increasingly complex ideas about the workings of the unconscious yielded research with the charm of mesmerists’ shows, revealing unlikely forces that seem to guide our behavior: that simply having people wash their hands could change their sense of culpability; that people’s evaluations of risk could easily be rendered irrational; that once people have made a decision, they curiously give more weight to information in its favor. Humans, the research often suggested, were reliably mercurial, highly suggestible, profoundly irrational, tricksters better at fooling ourselves than anyone else.
Already relatively accessible to the public, the field became even more influential with the rise of behavioral economics in the 1980s and 1990s, as visionaries like Richard Thaler, (who won the Nobel Prize in economics this month) found applications for counterintuitive social-psychology insights that could be used to guide policy. In 2000, Malcolm Gladwell, the author of the best-selling “Tipping Point,” applied irresistible storytelling to the science, sending countless journalists to investigate similar terrain and inspiring social psychologists to write books of their own. In 2006, Daniel Gilbert, a professor of psychology at Harvard, published the best seller “Stumbling on Happiness” — a book that tried to explain why we plan so poorly for our own future. That same year, TED started airing its first videos, offering a new stage for social psychologists with compelling findings, ideally surprising ones. The field was experiencing a visibility unknown since the midcentury; business schools, eager for social psychologists’ insights into leadership and decision-making, started pursuing social psychologists, with better pay and more funding than psychology graduate schools could offer.
This moment of fizziness for the discipline was already underway when Cuddy arrived at Princeton’s graduate program in 2000, transferring there to follow her adviser, Susan Fiske, with whom she first worked at the University of Massachusetts at Amherst. She moved to Princeton with her husband at the time — they later divorced — and in her second year there, she had a child. Her academic work continued to thrive as she collaborated with Fiske on research on stereotyping, which found that groups of people (for example of a particular ethnicity) who were judged as nice were assumed to be less competent and vice versa. (“Just Because I’m Nice Don’t Assume I’m Dumb,” was the headline of a Harvard Business Review article by Cuddy.) Fiske and Cuddy’s resulting papers are still heavily cited, formulating a framework for stereotyping that proved hugely influential on the field.
And yet, especially early on at Princeton, Cuddy felt uncertain of her place there. She feared that her brain simply could not function at a high-enough level to power her through the program. Cuddy suffered a traumatic brain injury in a car accident the summer after her sophomore year in college, when a friend of hers fell asleep at the wheel while Cuddy was asleep in the back seat. In the months after the accident, Cuddy was told she should not expect to finish college; her fog was so deep that she remembers being retaught how to shop for groceries. Even after Cuddy recovered, her friends told her that she had changed, that she was not the same person — but she could not remember who she had been before. It took her four years and multiple false starts before she could return to college. Even after she was accepted to graduate school at University of Massachusetts, she confessed to Fiske that she feared she would not be able to keep up with the work.
Cuddy was trained as a ballet dancer — in between her stints at college, she danced as an apprentice with the Colorado Ballet — but her interest in studying the body and its relationship to power did not begin until 2009, her first year as a teacher at Harvard Business School. At the invitation of her department chair, she joined a small circle of academics meeting with Joe Navarro, a former F.B.I. agent who had written a book about body language. The invited parties, which included Dana Carney, a professor at Columbia University who studied body language and power, all spoke briefly about their work. In the conversation that followed, Navarro pointed out that Cuddy’s own body language, during her presentation, signaled insecurity: She was fiddling with her necklace, wrapping her arms around her torso.
Carney and Cuddy had been noticing the same kinds of body language in their female students, who performed well on written materials but lost points, compared with their male counterparts, on class participation. Their body language was constricted; they raised their hands with their elbows cradled in their other hands; they made themselves physically small; they spoke less often.
The two researchers wondered whether people whose physical cues looked like their female students’ — self-protective, insecure — would feel more powerful or even change their behavior if they simply adopted more expansive body positions.
Carney and Cuddy brainstormed a research project to test this question. At Columbia, Carney recruited students, telling them that they were part of a study intended to measure the effects of placing an electrocardiograph’s electrodes either above or below the heart. In the study of 42 subjects that they eventually published, experimenters arranged half the students into positions associated with high power (leaning back in a chair with feet crossed on a desk, for example) and half the students into positions associated with low power (like crossing arms in front of the body). Before and after the poses, experimenters took saliva swabs from the students to measure how the positions affected cortisol and testosterone levels. They also asked the students to report (before and after the poses) how in charge and powerful they felt on a scale of one to four (a measurement known as “self-reported feelings of power”). And they measured whether that feeling had what researchers call a “downstream effect” — a resulting behavior. People who feel power, the literature suggests, are more likely to engage in a range of certain behaviors, including risk-taking; so the experiment also measured the subjects’ willingness to bet on a roll of the dice.
The year that Amy Cuddy published her power-posing paper, Joseph Simmons, who attended graduate school at Princeton with Cuddy, was starting to think about his own seminal paper, one that would, unknown to either of them, have as much influence on her life as it would on his own; it would, in fact, change not just their lives but the field as they knew it, with wildly differing consequences for each of them.
Cuddy and Simmons, each of whom came from working-class backgrounds, had been fond of each other at Princeton, even if they did not socialize often: Cuddy was a new mother, and Simmons was five years younger and heavily committed to his softball team. Simmons considered Cuddy a friend, someone he was always happy to see at a party, despite their obvious differences: Cuddy, who used to follow the Grateful Dead, would have been the one dancing at the party, while Simmons would have been the one laughing with his close friend, a fellow graduate student named Leif Nelson, about the latest buzzy journal article that seemed, to them, ridiculous.
After finishing a postdoctoral program at Princeton, Simmons lost touch with Cuddy, who was by then teaching at Northwestern. He remained close to Nelson, who had befriended a behavioral scientist, also a skeptic, Uri Simonsohn. Nelson and Simonsohn kept up an email correspondence for years. They, along with Simmons, took particular umbrage when a prestigious journal accepted a paper from an emeritus professor of psychology at Cornell, Daryl Bem, who claimed that he had strong evidence for the existence of extrasensory perception. The paper struck them as the ultimate in bad-faith science. “How can something not be possible to cause something else?” Nelson says. “Oh, you reverse time, then it can’t.” And yet the methodology was supposedly sound. After years of debating among themselves, the three of them resolved to figure out how so many researchers were coming up with such unlikely results.
Simmons called those questionable research practices P-hacking, because researchers used them to lower a crucial measure of statistical significance known as the P-value. The P stands for probable, as in: How probable is it that researchers would happen to get the results they achieved — or even more extreme ones — if there were no phenomena, in truth, to observe? (And no systematic error.) For decades, the standard of so-called statistical significance — also the hurdle to considering a study publishable — has been a P-value of less than 5 percent.
To examine how easily the science could be manipulated, Simmons and Simonsohn ran a study in which they asked 20 participants their ages (and their fathers’ birthdays). Half the group listened to the Beatles song “When I’m Sixty-Four”; the other listened to a control (the instrumental music “Kalimba”). Using totally standard methodology common to the field, they were able to prove that the participants who listened to the Beatles song were magically a year and a half younger than they were before they had heard the music. The subject heading of the explanation: “How Bad Can It Be? A Demonstration of Chronological Rejuvenation.” It was witty, it was relatable — everyone understood that it was a critique of the fundamental soundness of the field.
“We realized entire literatures could be false positives,” Simmons says. They had collaborated with enough other researchers to recognize that the practice was widespread and counted themselves among the guilty. “I P-hacked like crazy all through my time at Princeton, and I still couldn’t get interesting results,” Simmons says.
The paper generated its fair share of attention, but it was not until January 2012, at a tense conference of the Society for Personality and Social Psychology in San Diego, that social psychologists began to glimpse the iceberg looming ahead — the sliding furniture, the recriminations, the crises of conscience and finger-pointing and side-taking that would follow. At the conference, several hundred academics crowded into the room to hear Simmons and his colleagues challenge the methodology of their field. First, Leslie John, then a graduate student, now an associate professor at the Harvard School of Business, presented a survey of 2,000 social psychologists that suggested that P-hacking, as well as other questionable research practices, was common. In his presentation, Simonsohn introduced a new concept, a graph that could be used to evaluate bodies of research, using the P-values of those studies (the lower the overall P-values, the better). He called it a P-curve and suggested that it could be used, for example, to evaluate the research that a prospective job candidate submitted. To some, the implication of the combined presentations seemed clear: The field was rotten with the practice, and egregious P-hackers should not get away with it.
Journal-paper presentations on statistics are usually unremarkable affairs, but this one precipitated a sequence of exchanges so public that the field had to take notice. The first took place during that event, with Norbert Schwarz, an eminent social psychologist, in the audience. Schwarz, as he listened, grew furious: He believed that the methodology of the survey was flawed, and he indignantly objected to the idea of the P-curve as a kind of litmus test aimed at individuals. He would not let these ideas go uncontested; he interrupted loudly from the front row, violating standard academic etiquette. “The whole room was like, ‘Oh, my God, what just happened?’ ” recalls Brian Nosek, a social psychologist who now runs the Center for Open Science, intended to encourage and normalize replications. Others quietly thanked Schwarz for bravely speaking up.
Not content to stop there, Schwarz followed up four days later with an open letter to 5,000 members of the society’s listserv, explaining in further detail, and with some condescension, his reservations. Although Simonsohn was angry, he still hoped to cool down the conversation. He emailed Schwarz asking if they could talk, so that they could come to a sort of understanding, in the name of science, and release a joint statement. Schwarz agreed but told Simonsohn, over the course of several email exchanges, that he needed more time. Simonsohn lost patience after three weeks: He posted large parts of the email exchange on his personal website, then posted a blistering attack on Schwarz on the society’s listserv, filled with bold caps and underlines, in which he said, among other things, that he knew firsthand that Schwarz had engaged in P-hacking.
“I regret it,” Simonsohn says now about posting the emails. Since then, Simmons, Simonsohn and Nelson say they have given a lot of thought to codes of conduct for communicating responsibly when conveying concerns about a scientist’s work. But the academic blowup between Simonsohn, then a relative unknown in social psychology, and Schwarz, the standard-bearer, signaled from the beginning that leaders on each side would ignore the norms of scientific discourse in an effort to discredit the other. One imminent shift in methods would bring another shift — one of tone — that would affect the field almost as drastically.
For years, researchers treated journal articles, and their authors, with a genteel respect; even in the rare cases where a new study explicitly contradicted an old one, the community assumed that a lab error must account for the discrepancy. There was no incentive to replicate, in any case: Journals were largely not interested in studies that had already been done, and failed replications made people (maybe even your adviser) uncomfortable.
But in the years after that Society for Personality and Social Psychology conference, a sense of urgency propelled a generation of researchers, most of them under 40, to re-examine the work of other, more established researchers. And politeness was no longer a priority. “All of a sudden you have people emailing other people, asking for their data and then writing blog posts accusing them of shoddy practices,” says Eli Finkel, a social psychologist at Northwestern. “That was unheard-of. Now it was happening all the time.” Some blog posts took on the impact of journal articles, as interested parties weighed in with an impromptu peer review. In 2014, Psychological Science started giving electronic badges, an extra seal of approval, to studies that made their data and methodologies publicly available and preregistered their design and analysis ahead of time, so that researchers could not fish around for a new hypothesis if they turned up some unexpected findings.
Not surprisingly, replicators sometimes encountered the kind of outraged resistance that Simmons and Simonsohn initially did. The same month that Simmons and Simonsohn gave their talk, Stéphane Doyen, a social psychologist in Belgium, published a paper challenging a classic study in the field of priming, which holds that small cues, like exposure to certain words, can subconsciously trigger behaviors. The original study found that research subjects walked more slowly after being exposed to words associated with old age; the replicators found no such effect and titled their journal article “Behavioral Priming: It’s All in the Mind; but Whose Mind?” John Bargh, a professor at Yale, a luminary who published the original study, responded with a combative post on Psychology Today’s blog, claiming that discrepancies in the experiment design accounted for the difference and calling the researchers “incompetent or ill informed.” When other priming studies failed to replicate later that year, the Nobel laureate Daniel Kahneman, who discussed priming in his book “Thinking Fast and Slow,” wrote a letter to social psychologists who studied the effect, urging them to turn their attitude around. “To deal effectively with the doubts, you should acknowledge their existence and confront them straight on,” he wrote.
In August 2014, the day before her second marriage, Amy Cuddy learned that a replication of her 2010 study led by a 34-year-old economist at the University of Zurich named Eva Ranehill had failed to yield the same results. “I remember thinking, Oh, bummer,” Cuddy says. But she was not distraught; often there was some perfectly good reason for a discrepancy in two studies of the same concept.
There were several key differences — Ranehill’s sample size, at 200, was much bigger, and she had designed a double-blind setup. Ranehill had her subjects hold two poses for three minutes each. She did not find an increase in either risk-taking behavior or the expected hormone changes.
She was relieved to see that the “feelings of power” finding had replicated. But Ranehill used language in her write-up that played down that finding’s importance. Although in one study of her own, Cuddy also played down the finding, she has otherwise consistently, in interviews, been enthusiastic about the idea that a body posture could change someone’s feelings. “We’re psychologists,” she says. “We try to change how people feel.” She also, at the time of the Ranehill replication, still anticipated that other research would probably show downstream effects — more risk taking, or more competitiveness, or better performance in job interviews.
By the time Cuddy got word of Ranehill’s replication, she had given her TED talk, developed a significant speaking career and was writing a book. Simmons had received tenure at Wharton and was writing, with Simonsohn, a blog called Data Colada, in which they sometimes tried to replicate other people’s work. By 2014, there was near-unanimous agreement the Data Colada team had profoundly changed the field’s research techniques for the better. But for the average researcher, an email from someone at Data Colada signaled unpleasantness ahead. “It’s like the police knocking on your door in the middle of the night,” one psychologist said.
In the wake of Ranehill’s failed replication, Cuddy and Carney set to work on a response. Carney, who is now a tenured associate professor of management at the University of California, Berkeley, tried to chart a P-curve of all 33 studies they were mentioning in their paper (which was already under review). Carney sent the paper and the P-curve to Nelson for some feedback, but he sent it on to Simmons and Simonsohn, as they were the experts.
The letter Simmons wrote back to Carney was polite, but he argued that her P-curve had not been executed correctly. He and Simonsohn had each executed P-curves of the 33 studies, and each found that it was flat, suggesting that the body of literature it reflected did not count as strong evidence. He did write that “conceptual points raised before that section are useful and contribute to the debate” but that they should take the P-curve out. “Everybody wins in that case.” According to Cuddy, she and Carney thought the P-curve science was not as settled as Simmons believed it to be. But afraid of public recrimination, they did exactly as he said — they took out the P-curve.
The post criticized the new paper, as well as the 2010 study. It showed Simmons and Simonsohn’s own unfavorable P-curve and essentially argued that the original published findings on hormones and risk-taking were probably a result of chance. They did not include a feelings-of-power measure in the P-curve they showed. But the blog post did mention in its last footnote that there was a significant effect of power posing on “self-reported power,” although the language made it clear that it didn’t count for much: Simmons believes that self-reports of power generally reflect what is called a demand effect — a result that occurs when subjects intuit the point of the study. Cuddy believes that studies can be constructed to minimize that risk and that demand effects are often nuanced.
Cuddy responded to Simonsohn with a few points that they incorporated into the post but said she preferred to write a longer response in a context in which she felt more comfortable.
Cuddy felt ill when Simmons and Simonsohn published the post with the headline: “Reassessing the Evidence Behind the Most Popular TED Talk.” As illustration, they used a picture of Wonder Woman. Cuddy felt as if Simmons had set them up; that they included her TED talk in the headline made it feel personal, as if they were going after her rather than the work.
The post, which Simonsohn distributed to his email list of hundreds, quickly made the rounds. “People were sending me emails like I was dying of cancer,” Cuddy says. “It was like, ‘We send our condolences,’ ‘Holy crap, this is terrible’ and ‘God bless you; we wish we could do something, but obviously we can’t.’ ” She also knew what was coming, a series of events that did, in fact, transpire over time: subsequent scrutiny of other studies she had published, insulting commentary about her work on the field’s Facebook groups, disdainful headlines about the flimsiness of her research. She paced around, distraught, afraid to look at her email, afraid not to. She had just put together a tenure package and worried that the dust-up would be a continuing distraction.
Cuddy did not like seeing her work criticized in a non-peer-reviewed format, but she wrote a bland statement saying, essentially, that she disagreed with their findings and looked forward to “more progress on this important topic.” Carney reassured Cuddy in the months after the Data Colada post that their paper would eventually be vindicated — of course the effects were real; someone would prove it eventually.
Eventually, the Data Colada post caught the eye of another influential blogger, Andrew Gelman, a professor of statistics and political science at Columbia University, whose interest in Cuddy’s work would prove durable, exacting and possibly career-changing for Cuddy. Gelman wields his sizable influence on the field from afar, on his popular blog andrewgelman.com, where he posts his thoughts on best statistical practices in the sciences, with a frequent emphasis on what he sees as the absurd and unscientific. Gelman, who studied math and physics at M.I.T. before turning to statistics, does not believe that social psychology is any more guilty of P-hacking than, say, biology or economics. But he has devoted extensive attention to the field, especially in more recent years, in part because of the way the media has glorified social-psychology research. He is respected enough that his posts are well read; he is cutting enough that many of his critiques are enjoyed with a strong sense of schadenfreude.
Four months after the Data Colada post, Gelman, with a co-author, published an article in Slate about Carney and Cuddy’s 2010 study, calling it “tabloid fodder.” Eventually, Cuddy’s name began appearing regularly in the blog, both in his posts and in comments. Gelman’s writing on Cuddy’s study was coolly dismissive; it bothered him that Cuddy remained fairly silent on the replication and the Data Colada post. For all he knew, Cuddy was still selling the hormone effect in her speaking gigs and in her best-selling book, “Presence,” which he had not read. Had he looked, he would have been annoyed to see that Cuddy did not include a mention of the Ranehill replication. But he might have been surprised to see how little of the book focused on power posing (just a few pages).
On his site, Cuddy’s name, far from the only one he repeatedly invoked, became a go-to synecdoche for faulty science writ large. When he saw that Cuddy had been invited to speak at a conference, he wondered why the organizers had not invited a bunch of other famous figures he clearly considered bad for science, including Diederik Stapel, who had been accused of outright fraud.
His site became a home for frequently hostile comments from his followers. “She has no serious conception of ‘science,’ ” one posted. Another compared Cuddy to Elizabeth Holmes, the Theranos chief executive under investigation for misleading investors. Though Gelman did encourage his readers to stick to the science, he rarely reined anyone in. In one exchange in July 2016, a commenter wrote, “I’ve wondered whether some of Amy Cuddy’s mistakes are due to the fact that she suffered severe head trauma as the result of a car accident some years ago.” Gelman replied, “A head injury hardly seems necessary to explain these mistakes,” pointing out that her adviser, Fiske, whom he has also criticized, had no such injury but made similar errors.
Gelman considers himself someone who is doing others the favor of pointing out their errors, a service for which he would be grateful, he says. Cuddy considers him a bully, someone who does not believe that she is entitled to her own interpretation of the research that is her field of expertise.
Cuddy has asked herself what motivates Gelman. “Why not help social psychologists instead of attacking them on your blog?” she wondered aloud to me. “Why not come to a conference or hold a seminar?” When I asked Gelman if he would ever consider meeting with Cuddy to hash out their differences, he seemed put off by the idea of trying to persuade her, in person, that there were flaws in her work.
“I don’t like interpersonal conflict,” he said.
It was the kind of information Cuddy wished she did not have; her closest friends were told to stop passing on or commenting about that kind of thing, but acquaintances still did it. She felt adrift in her field. She worried about asking peers to collaborate, suspecting that they would not want to set themselves up for intense scrutiny. And she felt betrayed, not just by those who cut her down on social media, in blog posts, even in reviews (one reviewer called her “a profiteer,” not hiding his contempt), but also by some of those who did not publicly defend her. She was not wrong to think that at least in some cases, it was fear, rather than lack of support for her, that kept people from speaking up. Two tenured psychology professors at Ivy League universities acknowledged to me that they would have publicly defended some of Cuddy’s positions were they not worried about making themselves targets on Data Colada and elsewhere.
Two days before Cuddy received that text from a friend, Gelman once again posted about the power-posing research, but this time he issued a challenge to Dana Carney. “When people screw up or cheat in their research, what do their collaborators say?” he wondered in the post. For Carney, he wrote, “it was not too late.” Unknown to Cuddy and Gelman, Carney had already linked, in her C.V., to Simmon and Simonsohn’s critique of that first, influential 2010 study, but she hadn’t made the kind of statement or gesture that Gelman expected from Cuddy.
She listed a number of methodological concerns she had, in retrospect, about the 2010 paper, most of which, Cuddy says, Carney had never raised with her. In an email a few months earlier, Carney had clearly told Cuddy that she thought the study’s data was flimsy, the sample was tiny, the effects were barely there. But Cuddy said she had never received notice that this kind of renunciation was coming. Carney declined to comment for this article, but Nelson, who is in her department, said she was clearly in a tough position, saddled with all the negatives of the work — the hit to her reputation — with none of the upside: the speaking fees and the positive feedback from teary fans who no doubt fuel Cuddy’s conviction in the research.
For much of the scientific world, Carney’s statement was an act of integrity and bravery. “Whoa! This is how to do it!” tweeted Michael Inzlicht, a professor of psychology and neuroscience at the University of Toronto who had eloquently written about his own crisis of confidence about his field of research, ego depletion.
Cuddy wrote a lengthy response to Carney that New York magazine published. (New York, Slate and The Atlantic have closely reported on the replication movement.) Then she stopped taking phone calls and went almost completely offline. She found that she couldn’t eat; at 5-foot-5, Cuddy went down to 100 pounds.
Less than two weeks after Carney’s disavowal, Cuddy got on a plane so she could meet her commitment to speak to a crowd of 10,000 in Las Vegas. As frail as she had been since her accident, she headed to an arena in Las Vegas and roused the crowd, a tiny woman on a giant stage, taking up space, making herself big, feeling the relief of feeling powerful.
When Simmons and I met, I asked him why he eventually wrote such a damning blog post, when his initial correspondence with Carney did not seem particularly discouraging. He and Simonsohn, he told me, had clearly explained to Cuddy and Carney that the supporting studies they cited were problematic as a body of work — and yet all the researchers did was drop the visual graph, as if deliberately sidestepping the issue. They left in the body of literature that Simmons and Simonsohn’s P-curve discredited. That apparent disregard for contrary evidence was, Simmons said, partly what prompted them to publish the harsh blog post in the first place.
Every researcher has a threshold at which he or she is convinced of the evidence; in social psychology, especially, there is no such thing as absolute proof, only measures of probability. In recent months, Cuddy reached the threshold needed to alter her thinking on the effect of hormones. She mentioned, at a psychology conference where she was presenting her work, that a study had recently been conducted on power posing. “They found no hormonal effects,” she said before taking a breath. “That study is done very well, and I trust those results.” Although 11 new studies have recently been published that do not show the downstream effects of power posing on behaviors, Cuddy is still fighting for power posing. The research, she says, still shows its effect on feelings of power: At the conference, she presented a comprehensive meta-analysis, a version of which, she says, she will soon publish, with a strong P-curve supporting that, and she also presented a P-curve suggesting that power posing had a robust effect on self-evaluations, emotions and moods.
Cuddy now seems ready to move on to a new phase. We met near her home in Newton, Mass., in August. Cuddy, smiling, fresh from physical therapy for a torn ACL, was in a tennis skirt, looking young and more lighthearted than I had ever seen her. She had abandoned the dream of tenure. She was planning a new project, a new book, she told me. It was coming together in her mind: “Bullies, Bystanders and Bravehearts.” It would be personal; there would be research; she would write, and she would talk, and she would interview people who had suffered fates worse than her own and bounced back. She would tell their stories and hers, and because she is a good talker, people would listen.