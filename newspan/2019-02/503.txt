Technology|YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?
YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?
Innocent or not, Mr. Dawson’s videos contain precisely the type of viral misinformation that YouTube now says it wants to limit. And its effort raises an uncomfortable question: What if stemming the tide of misinformation on YouTube means punishing some of the platform’s biggest stars?
Part of the problem for platforms like YouTube and Facebook — which has also pledged to clean up misinformation that could lead to real-world harm — is that the definition of “harmful” misinformation is circular. There is no inherent reason that a video questioning the official 9/11 narrative is more dangerous than a video asserting the existence of U.F.O.s or Bigfoot. A conspiracy theory is harmful if it results in harm — at which point it’s often too late for platforms to act.
Take, for example, Mr. Jones’s assertion that the mass shooting at Sandy Hook Elementary School in 2012 was a hoax perpetrated by gun control advocates. That theory, first dismissed as outrageous and loony, took on new gravity after Mr. Jones’s supporters began harassing the grieving parents of victims.
Or take Pizzagate, a right-wing conspiracy theory that alleged that Hillary Clinton and other Democrats were secretly running a child-sex ring. The theory, which was spread in a variety of videos on YouTube and other platforms, might have remained an internet oddity. But it became a menace when a believer showed up at a pizza restaurant in Washington, D.C., with an assault rifle, vowing to save the children he believed were locked in the basement.
To its credit, YouTube has taken some minor steps to curb misinformation. Last year, it began appending Wikipedia blurbs to videos espousing certain conspiracy theories, and changed the way it handles search results for breaking news stories so that reliable sources are given priority over opportunistic partisans. And last summer, it was among the many social networks to bar Mr. Jones and Infowars.
Mr. Chaslot noted that this algorithm — which was once trained to maximize the amount of time users spend on the site — often targeted vulnerable users by steering them toward other conspiracy theory videos it predicts they will watch.
The change “will save thousands from falling into such rabbit holes,” he wrote.
In an interview last week, Mr. Chaslot was more circumspect, saying YouTube’s move may have amounted to a “P.R. stunt.” Because the change will affect only which videos YouTube recommends — conspiracy theories will still show up in search results, and they will still be freely available to people who subscribe to the channels of popular conspiracy theorists — he called it a positive but insufficient step.
“It will address only a tiny fraction of conspiracy theories,” he said.
Last year, Mr. Chaslot built a website, AlgoTransparency.org, to give outsiders a glimpse of YouTube’s recommendation algorithms at work. The site draws from a list of more than 1,000 popular YouTube channels, and calculates which videos are most often recommended to people who watch those channels’ videos.
On many days, conspiracy theories and viral hoaxes top the list. One recent day, the most frequently recommended video was “This Man Saw Something at Area 51 That Left Him Totally Speechless!,” which was recommended to viewers of 138 channels. The second most recommended video, which linked a series of recent natural disasters to apocalyptic prophecies from the Book of Revelation, was recommended to viewers of 126 of those top channels.
In our conversation, Mr. Chaslot suggested one possible solution to YouTube’s misinformation epidemic: new regulation.
Lawmakers, he said, could amend Section 230 of the Communications Decency Act — the law that prevents platforms like YouTube, Facebook and Twitter from being held legally liable for content posted by their users. The law now shields internet platforms from liability for all user-generated content they host, as well as the algorithmic recommendations they make. A revised law could cover only the content and leave platforms on the hook for their recommendations.
But even new laws governing algorithmic recommendations wouldn’t reverse the influence of YouTube celebrities like Mr. Dawson. After all, many of his millions of views come from his fans, who subscribe to his channel and seek out his videos proactively.
YouTube’s first challenge will be defining which of these videos constitute “harmful” misinformation, and which are innocent entertainment meant for an audience that is largely in on the joke.
But there is a thornier problem here. Many young people have absorbed a YouTube-centric worldview, including rejecting mainstream information sources in favor of platform-native creators bearing “secret histories” and faux-authoritative explanations.
When those creators propagate hoaxes and conspiracy theories as part of a financially motivated growth strategy, it seeps in with some percentage of their audience. And sometimes — in ways no algorithm could predict — it leads viewers to a much darker place.
It’s possible that YouTube can still beat back the flood of conspiracy theories coursing through its servers. But doing it will require acknowledging how deep these problems run and realizing that any successful effort may look less like a simple algorithm tweak, and more like deprogramming a generation.