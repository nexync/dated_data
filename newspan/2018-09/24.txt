The Upshot|Why Did We Do the Poll the Way We Did?
Why Did We Do the Poll the Way We Did?
A look at the challenges and trade-offs of conducting a political survey.
Read more about the methodology of how the poll was conducted.
Not every pollster does this. Most national polls call telephone numbers at random, using a technique called random digit dialing.
In the abstract, there’s a fine debate to be had about which approach is superior. But we had no choice: It is hard or even impossible to conduct a poll of a congressional district with random digit dialing, since no lists exist of telephone numbers sorted by congressional district.
Calling voters from the voter file also has advantages, because it means we know a lot about our respondents before we’ve even spoken with them. In most states, we have their age, their gender, a reliable indicator of partisanship like party registration, and a record of when they voted (though not for whom). We know where they live, so we know whether their neighborhoods voted for Hillary Clinton or Donald J. Trump. We know their names, which along with their neighborhoods can give us a good idea of their race.
We can use this information in many ways — most important, to make sure our sample is representative. We know how many registered Democrats or Republicans live in the district, so we can make sure we have the right proportion of each in our poll. We can also use this data to model the likely electorate because past turnout is a pretty good predictor of future turnout.
The voter file has two crucial disadvantages. The biggest one is that not all the people in the voter file have a telephone number associated with their record (in contrast, random digit dialing would ensure that every person had a chance of being called). If those individuals are different, politically, than seemingly similar people with a telephone number, our polls would be biased.
This is a real concern, but not as big a problem as it used to be. Voter files have improved, and we have telephone numbers for a majority of registered voters. At the same time, diminishing response rates have made the coverage issue seem less important. Today, no poll can really claim something near theoretical purity when 90 percent of people decline to take a poll.
The other major disadvantage is that the voter file lacks reliable information on the educational attainment of registered voters. This is a pretty big deal in the Trump era, and it’s part of why the polls overestimated Mrs. Clinton’s standing in 2016. College-educated voters are particularly likely to respond to polls, so pollsters need to adjust their samples to ensure they’re representative of the overall educational composition of the electorate.
On this front, the random-digit-dialing pollsters have it easy: They call everyone, so they can adjust their samples to match census data on the educational makeup of the country. The registration-based-sample pollsters struggle: Without a measure of education on the voter file, it’s hard to make a proper adjustment, and the census doesn’t report education among registered voters. It’s a real challenge, but as we will explain, it’s a surmountable one.
How did we decide whom to call?
We call voters off the voter file, but we don’t call just anyone. We do two things that are pretty unusual among public pollsters, but common in the world of campaign polling.
This matters because most public pollsters, in contrast, take a random sample and wind up with too few young, less educated and low-turnout voters. To compensate, they adjust their samples to give more weight to respondents from underrepresented groups — a process called weighting.
We weight our poll, too, which we’ll get to in a bit. But we call more voters from low-response-rate groups on the front end, which reduces the need for weighting. Although this makes our poll more expensive, it means we have a proper sample of groups who matter a lot in politics, but who are not always well represented in polls. In our postelection analysis, we found that this was part of why our 2016 polls reflected stronger support for Mr. Trump than some other polls did — or at least were more accurate than they would have been otherwise. Low-turnout voters tend to be less educated, so this step also lessened one of the big challenges of the cycle.
That said, the advantages of this approach have declined since 2016. More and more voters on the voter file have a cellphone number, and cellphone respondents tend to be more representative.
The advantage of polling the likely electorate is straightforward: It’s cheaper in a low-turnout election, and you’re talking to the people who are likeliest to vote.
The disadvantage is also straightforward: You put a lot of trust in your turnout model, and there’s not much you can do if it’s wrong. Obviously, every poll relies on its turnout model. But here you’re letting it choose whom you call, in addition to whether they’ll vote.
This is part of why campaign polls sometimes totally miss surprising candidates like Alexandria Ocasio-Cortez in New York’s 14th District. In a race like that, pollsters might rely on some kind of standard primary turnout model, and never even call many of the politically engaged but irregular voters who showed up to put her over the top.
What makes this particularly challenging is that the risks and rewards of relying on a likely electorate go up and down at the same time. The advantages and disadvantages are greatest in a low-turnout primary, and lowest in a presidential year. A midterm election is somewhere in the middle. In 2016, we called all registered voters.
So why are we doing it this way?
In part, because we already have a sample design element that increases costs and improves the representation of generally underrepresented groups: the response rate adjustment. To some extent, these choices cancel each other out.
We’re also doing enough polls, far enough in advance, that we should be able to detect whether a much larger than expected share of seemingly low-turnout respondents intends to vote, and assess how much different our results would be if they had an equal chance of selection at the start. If the choice seems problematic, we can reverse ourselves.
Another reason is that our method isn’t as blunt and as risky as some. There are a lot of pollsters, for instance, who call only people who have voted in two of the last four elections. They miss people entirely; we don’t cut out anyone out like that.
And maybe most important, our analysis of prior election data and our own prior polls indicate that we should probably trust our turnout model more than anything else. As a result, we wouldn’t take as much advantage of the theoretical benefit of calling all registered voters anyway.
How did we model turnout?
As the last section implies, our turnout model plays a far bigger role in our survey than it does in most public polls. It determines whom we call. How we adjust our sample. And, of course, it has a big role in determining exactly who we think is likely to vote.
Turnout among young, nonwhite and less educated voters is consistently lower relative to older, white and more educated voters. The scale of the drop-off is also quite consistent. And nationally, midterm turnout has hovered in a pretty consistent range since those aged 18, 19 and 20 were enfranchised to vote in 1971: a high of 42 percent of eligible voters and a low of 36 percent in 2014, with all but 2014 landing between 38 percent and 42 percent.
On average, registered Republicans have a turnout advantage in midterm elections. This is probably mainly because of demographics, but the G.O.P. advantage in historical survey data, like the ANES, persists after controlling for standard demographic characteristics.
The party of the president is typically at a turnout disadvantage compared with the party out of power. In practice, this means that Republicans have had a really big turnout advantage in years when Democrats have held the presidency, while Democrats and Republicans have basically fought to parity in years when the Republicans held the presidency.
If you project these patterns forward to 2018, you’d anticipate Democratic turnout would surge to basically catch up to Republican turnout. That’s exactly been the pattern in the special and regularly scheduled elections held since Mr. Trump won the presidency. The electorate remains just as expected in terms of age and racial makeup, yet Democratic turnout has surged to approximately match Republican turnout, based on voter file records in New Jersey, Virginia, Kansas 4, Pennsylvania 18, Ohio 12, Georgia 6 and Arizona 8.
In those contests, 60 percent of Democrats (defined by either party registration or primary vote history) have voted compared with 58 percent of Republicans. In these same states, the Republicans had an eight-point turnout edge in 2014, with 62 percent voting vs. 54 percent of Democrats.
This closely matches the historical pattern, though the outright Democratic turnout advantage is unprecedented in the modern era. Some and maybe all of the gap is a result of an apples-to-oranges comparison between three measures of partisanship: self-reported party identification in polls, party registration in historical voter file data, and either party registration or primary vote history in the 2018 data.
In particular, the use of primary vote history in Virginia, Ohio and Georgia, where there’s no party registration, may exaggerate the Democratic turnout advantage. Past Democratic primary voters are a narrow group. They’re better educated and more politically engaged than the broader group of self-identified or registered partisans, and right now they’re particularly likely to identify as Democrats (rather than simply be registered as such), because in these districts in recent years, a lot more Democratic-leaners have been voting in Republican primaries than the other way around.
In states with party registration, a more broadly held and more stable characteristic that’s more directly comparable to the historical data, the Republicans have held a 1.2-point turnout edge, 48.7 percent to 47.5 percent. Importantly, the Democrats have the turnout edge in those same states among the narrower, better-educated and more reliably Democratic group of primary voters, 70 percent to 66 percent.
So, broadly speaking, the special and general elections tell a story that’s highly consistent with the historical record. This consistency makes it reasonable to expect that Democratic turnout in 2018 will increase to approximately match Republican turnout among registrants, and probably surpass it in the states where partisanship is measured by primary vote history.
But this clear story obscures a lot of underlying variation and the narrow character of the Democratic surge, which is concentrated among young and well-educated voters. This suggests that the clear national story might not play out so easily in individual states and districts. Turnout could vary a lot by district, ranging from well-educated, white districts where the Democrats could claim a shattering turnout advantage, to less educated and relatively diverse districts where Republicans could maintain their edge.
It is worth noting that Republican turnout looks pretty normal. It is not low. It doesn’t even really look different (though I could argue that turnout among well-educated Republican women looks weak). The change is all happening on the Democratic side and, to a lesser extent, among unaffiliated voters in well-educated and Democratic-leaning areas.
The combination of flat Republican turnout and a Democratic turnout surge implies a big increase in turnout over 2014. This isn’t a huge surprise: Turnout in 2014 was the lowest of the post-youth enfranchisement era, and by a pretty meaningful margin. But it does need to be accounted for.
A “general” midterm turnout model is fit using data from the 2014 midterm elections. The model predicts turnout in the midterm election as a function of a bunch of standard characteristics, the most important being whether someone has voted in recent presidential, midterm or primary elections.
Turnout in post-Trump elections is modeled as a function of the generic midterm turnout model described earlier, and various partisan and geographic characteristics like living in a well-educated census tract that voted for Mrs. Clinton. This model is pretty simple, and it omits a lot of the variables described in the first model, like history of voting in a recent midterm election. As a result, this model is probably best understood as adjusting the 2014 midterm model to account for changes in partisan turnout.
The turnout model is applied to a district’s registered voters, and those estimates are adjusted to match the turnout, by precinct, implied by the Virginia governor’s race. These estimates are adjusted to account for whether races on the statewide ballot would lead turnout to be higher or lower than it was in Virginia, based on a model of turnout in the 2005, 2006, 2009, 2010, 2013 and 2014 general elections.
Over all, this implies a national turnout of about 41 percent. This is at the upper end of the post-1972 turnout range, which peaked at 42 percent, and well above the 36 percent from 2014.
Obviously, voters will have the final say in how close these projections will be in reality. In general, the historical data gives us confidence in the broad turnout picture. The details, however, are dicier. It is hard to know, state by state or district by district, how turnout will play out.
One particularly big challenge is thinking about how much to care about the specific details of a district’s turnout in the last election. As currently configured, each district’s turnout is modeled separately, on the assumption that the voters of a district are unique enough to merit it. If nothing else, there’s a ton of variance in the available data by state. That makes it difficult to develop a one-size-fits-all approach, especially given the computational power needed to do it well. But the state-based approach has the disadvantage of tending to treat what happened in the last election as a neutral baseline.
Imagine, for instance, that the Democratic turnout was particularly weak in Texas last time because there was no competitive race on the ballot, or particularly strong in Kentucky because Democrats were motivated to defeat Mitch McConnell. Our approach deals with this in the aggregate: We believe turnout will surge in Texas, but drop in Kentucky. And, in both states, we assume that Democratic turnout will surge, relative to Republicans, in the ways described earlier.
But our approach does not specifically account for the prior partisan turnout gap, which may create a modest risk of underestimating the Democrats in a state where the prior Democratic baseline is particularly weak, like Texas, and some risk of overestimating them in a state where the prior Democratic baseline is particularly strong, like Kentucky.
Based on the limited historical data at our disposal, it is not obvious how to balance the advantages of incorporating the peculiarities of a given state’s data with the disadvantage of inheriting the competitiveness of recent elections that might not be so relevant.
It’s worth noting that we’re not completely helpless if we turn out to be off. We ask voters whether they intend to vote, and we incorporate that information into our final result. So, in theory, if we’re underestimating Democratic turnout in Texas, a disproportionate number of our low-turnout Democratic respondents will tell us that they intend to vote, and that would nudge our results in the right direction.
How do we weight?
The turnout model yields an individual-by-individual estimate of likely turnout. Aggregated together, those individual estimates add up to our estimates of the makeup of the likely electorate.
We use those estimates in trying to make sure that our poll is representative. If it’s not, we compensate by giving more weight to voters from underrepresented groups, a process known as weighting.
All pollsters weight in some way or another, and the choices matter. In 2016, the failure of many state pollsters to account for education was probably a reason they underestimated Mr. Trump’s support. Beyond that, even very modest differences in weighting can make a difference: Sometimes, you can change a poll result just by deciding whether you’ll weight by those age 18 to 29 or those 18 to 34.
In general, we follow a pretty standard set of weights and weight categories. We weight by age, party, gender, region, race, likely turnout and education.
Other than education, those estimates all come from the aggregated, individual-level turnout estimates in the voter file. So when we call people, we make sure we’re talking to the same person listed on the voter file and we weight by their voter file data, not by what they tell us.
On education, we have to take a different approach. The voter file doesn’t include a reliable indicator of a person’s education. This is a big reason many state polls simply didn’t weight by education at all in 2016.
The options to deal with this are pretty limited, in part because the “true” educational composition of the electorate is a mystery. There is no authoritative measure.
The exit polls, like 2016’s pre-election polls, aren’t weighted by education, and so they’re just as biased toward a well-educated electorate. The best option would seem to be the C.P.S., the census Current Population Survey. It has the same data that the jobs report is based on. After every general election, the survey tacks on some questions about voting and turnout.
But there are drawbacks. Most obviously, the survey doesn’t report a turnout estimate for each congressional district. And education levels might be off: The C.P.S. civilian non-institutional population, the universe represented by the C.P.S., reflects higher education levels (31.9 percent of adults have at least a bachelor’s degree) than the civilian non-institutional population in the American Community Survey (29.7 percent). The A.C.S. is a much larger census survey and is generally considered the gold standard of the demographic composition of the population.
Most national polls weight their samples to match American Community Survey targets for the education of the adult population. As a result, they wind up with a lower share of the electorate that is college educated than the C.P.S. does. A Pew study of validated voters on its American Trends Panel found a similar electorate.
Our estimates essentially combine Current Population Survey turnout rates with American Community Survey population data reflecting less education — and yield an electorate that looks a lot like the Pew and random-digit-dialing data that’s weighted to match A.C.S. targets.
Like our voter-file-based model, we also adjust these estimates to account for the consistent evidence of a turnout surge among well-educated voters. We use the validated turnout in Upshot/Siena poll data from Virginia and Ohio 12 to try to tease out how much the turnout among well-educated voters has increased since 2014. The effect of this adjustment is to increase the college-educated share of the electorate by two to three percentage points over a 2014-based model.
How do we decide who’s a likely voter?
So now we have a weighted sample of the likely electorate, based on our estimates for the likely electorate. We could call the poll done at this point. A lot of campaign pollsters would. But we go a step further: We incorporate the self-reported turnout intention of our respondents.
In principle, this could help us out if our turnout model is off. It’s worth noting, however, that this step wouldn’t have done much to help our polls in the past, even in spots where we would hope it would have, like in Virginia last year.
The way it works is pretty simple: If respondents seem more likely to vote than we thought they would be, we give them more weight in our poll. If they seem less likely, we give them less weight.
We do this based on how well respondents have been able to predict their own turnout in prior Upshot/Siena polls. One nice thing about polling off a voter file is that we can go back after the election and see who voted and who didn’t.
Over all, the results indicate that people do a pretty good job of predicting their own turnout. Yes, most of the people who say they’re “almost certain to vote” wind up voting. But not all. And yes, most of the people who say they’re “not very likely to vote” don’t turn out, but some do.
As a result, we don’t draw a hard line between “likely” and “unlikely” voters. Everyone, in our view, has some probability of voting.
One interesting thing that complicates this analysis is that poll respondents are much likelier to vote than nonrespondents, even after controlling for everything else. In other words, if we thought you had a 10 percent chance of voting before you picked up the phone, we instantly think you have a 35 percent chance of voting the moment you agree to take a political survey. That decision alone reveals that you’re really likelier to vote than your demographics and history of voting would suggest. After accounting for this phenomenon, we wind up giving a smaller bonus to unlikely voters who say they’re going to vote, simply because we don’t treat them as if they were all that unlikely to vote in the first place.
Our analysis of validated turnout in prior Upshot/Siena polls indicates that our models do a much better job of predicting turnout than self-reported intention. If we think you will vote, and you say you won’t, you’re probably still going to vote. But there is still some value in this data: If we weren’t sure you were going to vote, we do care quite a bit about what you told us.
Maybe it should be no surprise that self-reported turnout fared a little better in a special election than in regularly scheduled elections, where it’s presumably easier to model turnout using historical data.
Or maybe it’s a harbinger of what’s going to happen this November, and our turnout models won’t be as useful as they’ve been in the past.
Either way, we’ll show you all of these possibilities on our page. No matter what happens, we hope this is a transparent look into why polling works and why sometimes it does not.