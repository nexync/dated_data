The repercussions of the social media companies’ inability to handle disinformation and hate speech have manifested themselves abundantly in recent days. Cesar Sayoc Jr., who was charged last week with sending explosive devices to prominent Democrats, appeared to have been radicalized online by partisan posts on Twitter and Facebook. Robert D. Bowers, who is accused of killing 11 people at the Tree of Life synagogue in Pittsburgh on Saturday, posted about his hatred of Jews on Gab, a two-year-old social network.
The effects of social media were also evident globally. Close watchers of Brazil’s election on Sunday ascribed much of the appeal of the victor, the far-right populist Jair Bolsonaro, to what unfolded on social media there. Interests tied to Mr. Bolsonaro’s campaign appeared to have flooded WhatsApp, the messaging application owned by Facebook, with a deluge of political content that gave wrong information on voting locations and times, provided false instructions on how to vote for particular candidates and outright disparaged one of Mr. Bolsonaro’s main opponents, Fernando Haddad.
Elsewhere, high-ranking members of the Myanmar military have used doctored messages on Facebook to foment anxiety and fear against the Muslim Rohingya minority group. And in India, fake stories on WhatsApp about child kidnappings led mobs to murder more than a dozen people this year.
Social media companies have said that identifying and removing hate speech and disinformation — or even defining what constitutes such content — is difficult. Facebook said this year that only 38 percent of hate speech on its site was flagged by its internal systems. In contrast, its systems pinpointed and took down 96 percent of what it defined as adult nudity, and 99.5 percent of terrorist content.
A study by researchers from M.I.T. that was published in March found that falsehoods on Twitter were 70 percent more likely to be retweeted than accurate news.
But even as the companies throw money and resources at the problems, some of their employees said on Monday that they were rethinking whether the social media services could have a positive effect.
At Twitter, for example, employees are increasingly concerned that the company is floundering in its treatment of toxic language and hate speech, said four current and former employees who asked for anonymity because they had signed nondisclosure agreements.
The employees said their uncertainty surfaced in August, when Apple and other companies erased most of the posts and videos on their services from Alex Jones, the conspiracy theorist and founder of the right-wing site Infowars — but Twitter did not. (Twitter only followed suit weeks later.) Saturday’s shooting at the Pittsburgh synagogue led employees to urge Twitter’s leadership to firm up a policy on how to deal with hate speech and white supremacist content, two of the people said.
Twitter did not address questions about its employee concerns on Monday, but said it needed to be “thoughtful and considered” in its policies.
Instagram, which was created as a site for people to share curated photos of their food, adorable pets and cute children, has largely avoided scrutiny over disinformation and hate content — especially when compared with its parent, Facebook. But social media researchers said that the site had over the last year become more of a hotbed for hateful posts and videos meant to provoke discord.
That was evident after the Pittsburgh synagogue shooting, with the mushrooming of new anti-Semitic content on the site. On Sunday, one new video added to Instagram claimed that the state of Israel was created by the Rothschilds, a wealthy Jewish family. Underneath the video, the hashtags read #conspiracy and #jewworldorder.
By late Monday, it had been viewed more than 1,640 times and shared to other social media sites, including Twitter and Facebook.